{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dlppoh.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "enplmcsCp_ha"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX0fHSr3HI_6",
        "outputId": "18a58e6a-a4da-4676-ca4a-be22d4775bad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/rezer0dai/dlppoh\n",
        "!git clone https://github.com/fgolemo/gym-ergojr\n",
        "!git clone https://github.com/qgallouedec/panda-gym\n",
        "\n",
        "!pip install -e gym-ergojr\n",
        "!pip install -e panda-gym\n",
        "!pip install timebudget"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'dlppoh'...\n",
            "remote: Enumerating objects: 99, done.\u001b[K\n",
            "remote: Counting objects: 100% (99/99), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 99 (delta 37), reused 57 (delta 11), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (99/99), done.\n",
            "Cloning into 'gym-ergojr'...\n",
            "remote: Enumerating objects: 1227, done.\u001b[K\n",
            "remote: Total 1227 (delta 0), reused 0 (delta 0), pack-reused 1227\u001b[K\n",
            "Receiving objects: 100% (1227/1227), 43.11 MiB | 28.94 MiB/s, done.\n",
            "Resolving deltas: 100% (920/920), done.\n",
            "Cloning into 'panda-gym'...\n",
            "remote: Enumerating objects: 465, done.\u001b[K\n",
            "remote: Counting objects: 100% (465/465), done.\u001b[K\n",
            "remote: Compressing objects: 100% (298/298), done.\u001b[K\n",
            "remote: Total 465 (delta 236), reused 343 (delta 138), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (465/465), 9.81 MiB | 24.93 MiB/s, done.\n",
            "Resolving deltas: 100% (236/236), done.\n",
            "Obtaining file:///content/gym-ergojr\n",
            "Requirement already satisfied: gym>=0.2.3 in /usr/local/lib/python3.7/dist-packages (from gym-ergojr==1.3) (0.17.3)\n",
            "Collecting pybullet>=1.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/6d/60b97ffc579db665bdd87f2cb47fe1215ae770fbbc1add84ebf36ddca63b/pybullet-3.1.7.tar.gz (79.0MB)\n",
            "\u001b[K     |████████████████████████████████| 79.0MB 36kB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from gym-ergojr==1.3) (0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym-ergojr==1.3) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gym-ergojr==1.3) (4.41.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from gym-ergojr==1.3) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gym-ergojr==1.3) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.2.3->gym-ergojr==1.3) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.2.3->gym-ergojr==1.3) (1.5.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->gym-ergojr==1.3) (0.22.2.post1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gym-ergojr==1.3) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gym-ergojr==1.3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gym-ergojr==1.3) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gym-ergojr==1.3) (2.4.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.2.3->gym-ergojr==1.3) (0.16.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->gym-ergojr==1.3) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->gym-ergojr==1.3) (1.15.0)\n",
            "Building wheels for collected packages: pybullet\n",
            "  Building wheel for pybullet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pybullet: filename=pybullet-3.1.7-cp37-cp37m-linux_x86_64.whl size=89750446 sha256=921dc7bd564b4d517862bed34e5d604b7f9a6726397d17cdc41b118dfd0a0a4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/56/e6/fce8276a2f30165f7ac31089bb72f390fa16b87328651e1a5a\n",
            "Successfully built pybullet\n",
            "Installing collected packages: pybullet, gym-ergojr\n",
            "  Running setup.py develop for gym-ergojr\n",
            "Successfully installed gym-ergojr pybullet-3.1.7\n",
            "Obtaining file:///content/panda-gym\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from panda-gym==1.1.0) (0.17.3)\n",
            "Requirement already satisfied: pybullet in /usr/local/lib/python3.7/dist-packages (from panda-gym==1.1.0) (3.1.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from panda-gym==1.1.0) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym->panda-gym==1.1.0) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->panda-gym==1.1.0) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->panda-gym==1.1.0) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->panda-gym==1.1.0) (0.16.0)\n",
            "Installing collected packages: panda-gym\n",
            "  Running setup.py develop for panda-gym\n",
            "Successfully installed panda-gym\n",
            "Collecting timebudget\n",
            "  Downloading https://files.pythonhosted.org/packages/95/78/438d8f88adbb11b1cedc1525da9dfad9fc3b378d9e7391958ae234f8f502/timebudget-0.7.1-py3-none-any.whl\n",
            "Installing collected packages: timebudget\n",
            "Successfully installed timebudget-0.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unEYPXkVJdGE"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"dlppoh\")\n",
        "sys.path.append(\"gym-ergojr\")\n",
        "sys.path.append(\"panda-gym\")\n",
        "sys.path.append(\"/content/dlppoh\")\n",
        "sys.path.append(\"/content/gym-ergojr\")\n",
        "sys.path.append(\"/content/panda-gym\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrsg1IhfFP7p"
      },
      "source": [
        "import torch\n",
        "torch.set_default_dtype(torch.float32)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pRLFULDFP75"
      },
      "source": [
        "import numpy as np\n",
        "import random, timebudget"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_02ho3HYFP77"
      },
      "source": [
        "import config"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pucIFKuNFP8A"
      },
      "source": [
        "class Info:\n",
        "    def __init__(self, states, rewards, actions, custom_rewards, dones, goals, goods, pi):\n",
        "        self.states = states\n",
        "        self.rewards = rewards\n",
        "        self.actions = actions\n",
        "        self.custom_rewards = custom_rewards\n",
        "        self.dones = dones\n",
        "        self.goals = goals\n",
        "        self.goods = goods\n",
        "        self.pi = pi"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wv4i_24kFP8E"
      },
      "source": [
        "ones = lambda *shape: torch.ones(*shape)\n",
        "zeros = lambda *shape: torch.zeros(*shape)\n",
        "\n",
        "tensor = lambda x, shape: torch.tensor(x).view(*shape)\n",
        "\n",
        "achieved_goals = lambda states: states[:, :config.CORE_ORIGINAL_GOAL_SIZE]\n",
        "\n",
        "MOVE_DIST = 3e-4 \n",
        "moved = lambda s1, s: MOVE_DIST < torch.norm(s1 - s)\n",
        "\n",
        "def select_exp(states_1, states):\n",
        "    if not config.SELECT_EXP:\n",
        "        return [True] * len(states)\n",
        "    return [moved(s1, s) for s1, s in zip(achieved_goals(states_1), achieved_goals(states))]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r7c2ctbFP8H"
      },
      "source": [
        "from tasks.oaiproc import GymGroup, GymRender\n",
        "\n",
        "class LowLevelCtrlTask:\n",
        "    def __init__(self, dock, prefix):\n",
        "        self.ENV = GymGroup(config.TEST_ENVS, dock, config.TOTAL_ENV, prefix)\n",
        "        self.RENDER = GymRender(config.ENV_NAME, config.TOTAL_ENV)\n",
        "        \n",
        "        self.goals = None\n",
        "        self.info = None\n",
        "        self.set_goods(False)\n",
        "\n",
        "    def set_goods(self, goods):\n",
        "        self.goods = goods\n",
        "\n",
        "    def get_info(self):\n",
        "        return self.info\n",
        "        \n",
        "    def set_goals(self, goals):\n",
        "        self.goals = goals\n",
        "\n",
        "    def get_goals(self):\n",
        "        return self.goals\n",
        "\n",
        "    def optimal_goals(self, base_states, end_states):\n",
        "        base_states[:, -config.CORE_ORIGINAL_GOAL_SIZE:] = achieved_goals(self.info.states)\n",
        "        dist, _, _ = self.hl_agent.exploit(\n",
        "            end_states[:, :config.CORE_GOAL_SIZE],\n",
        "            base_states,\n",
        "            zeros(len(self.info.states), 1), 0)\n",
        "        return dist.sample()\n",
        "    \n",
        "    # time feature is only for critic\n",
        "    def append_time_feat(self, states):\n",
        "        self.n_steps += 1\n",
        "        if not config.TIMEFEAT:\n",
        "            return states\n",
        "        assert self.n_steps <  (1. + config.HRL_HIGH_STEP * config.HRL_STEP_COUNT)\n",
        "        tf = ones(states.shape[0], 1) - (self.n_steps /  (1. + config.HRL_HIGH_STEP * config.HRL_STEP_COUNT))\n",
        "        return torch.cat([states, tf], 1)\n",
        "\n",
        "    def _state(self, \n",
        "            einfo, actions, pi,\n",
        "            learn_mode=False, reset=False, seed=None):\n",
        "\n",
        "        states, goals, rewards, dones = einfo\n",
        "\n",
        "        states = self.append_time_feat(states)\n",
        "        \n",
        "        states = torch.cat([states, goals], 1)\n",
        "        if config.CORE_GOAL_SIZE != config.CORE_ORIGINAL_GOAL_SIZE:\n",
        "            goals = np.concatenate([goals, self.orig_pos], 1)\n",
        "\n",
        "        goods = self.goods if self.goods is not None else [False for _ in range(len(states))] \n",
        "        rewards = tensor(rewards, [-1, 1])\n",
        "        self.info = Info(\n",
        "                states,\n",
        "                rewards,\n",
        "                actions,\n",
        "# custom rewards here does not matter, all experience will be \"dreamed\" but based on true exp\n",
        "                rewards,\n",
        "                tensor(dones, [len(dones), -1]).float(),\n",
        "                self.goals,\n",
        "                goods,\n",
        "                pi,\n",
        "                )\n",
        "        \n",
        "        return self.info\n",
        "    \n",
        "    def reset(self, agent, seed, learn_mode):\n",
        "        return self.info\n",
        "\n",
        "    def internal_reset(self, agent, seed, learn_mode):\n",
        "        self.hl_agent = agent\n",
        "        self.n_steps = 0\n",
        "        self.learn_mode = learn_mode\n",
        "\n",
        "        if self.learn_mode:\n",
        "            einfo = self.ENV.reset(seed)\n",
        "        else:\n",
        "            einfo = self.RENDER.reset(seed)\n",
        "            \n",
        "        self.set_goods(None)\n",
        "    \n",
        "        self._state(\n",
        "            einfo, \n",
        "            None, None,\n",
        "            learn_mode, True, seed)\n",
        "        \n",
        "        return einfo[1]\n",
        "    \n",
        "    def step(self, pi):\n",
        "        if self.learn_mode:\n",
        "            einfo = self.ENV.step(\n",
        "                    pi[:, :pi.shape[1]//3].cpu().numpy(), ones(len(pi)))# if sum(self.goods) else None)\n",
        "        else:\n",
        "            einfo = self.RENDER.step(\n",
        "                    pi[:, :pi.shape[1]//3].cpu().numpy())\n",
        "\n",
        "        return self._state(einfo, pi[:, :config.ACTION_SIZE], pi, self.learn_mode, False)\n",
        "\n",
        "    def goal_met(self, _total_reward, _last_reward):\n",
        "        return False"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sVml8zQFP8Q"
      },
      "source": [
        "from config import *\n",
        "from timebudget import timebudget\n",
        "\n",
        "from utils.her import HER, CreditAssignment"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRGnro3MFP8T"
      },
      "source": [
        "COUNTER = [0, 0]\n",
        "\n",
        "SENTINEL = 2 # skip +1 last state dummy n_state it has, and last with valid n_state to use fast low level has +1 state we can throw when bit step_count ( 10 + ) and not waste too much experience\n",
        "class ReacherHRL(HER):\n",
        "    def __init__(self, cind, her_delay, gae, n_step, floating_step, gamma, gae_tau, her_select_ratio=.4, resampling=False, kstep_ir=False, clip=None):\n",
        "        super().__init__(cind, her_delay, gae, n_step, floating_step, gamma, gae_tau, her_select_ratio, resampling, kstep_ir, clip)\n",
        "\n",
        "    def _her_select_idx(self, n_steps):\n",
        "        hers = [ i if random.random() < .5 else 0 for i, s in enumerate(n_steps[:-SENTINEL]) ]\n",
        "        return hers + [0 for _ in range(SENTINEL)]\n",
        "\n",
        "    @timebudget\n",
        "    def update_goal(self, rewards, goals, states, states_1, n_goals, n_states, actions, her_step_inds, n_steps, allowed_mask):\n",
        "        global COUNTER\n",
        "\n",
        "        align = lambda x: x//config.HRL_STEP_COUNT*config.HRL_STEP_COUNT\n",
        "\n",
        "        MAX_HER_STEP = 1\n",
        "\n",
        "        h_goals = goals.clone()\n",
        "        h_n_goals = n_goals.clone()\n",
        "        h_rewards = rewards.clone()\n",
        "\n",
        "        h_states = states.clone()\n",
        "        h_n_states = n_states.clone()\n",
        "\n",
        "        assert not allowed_mask[-1] # last goal will be not used!!\n",
        "        allowed_mask[-2] = False # last state_1 should not be used too\n",
        "\n",
        "        idxs = []\n",
        "        her_goals = []\n",
        "        her_states = []\n",
        "\n",
        "        x = 0\n",
        "        z = 0\n",
        "\n",
        "        hers = []\n",
        "        others = []\n",
        "\n",
        "        COUNTER[sum(her_step_inds) > 0] += 1\n",
        "        for i in range(HRL_STEP_COUNT, len(goals)-1, HRL_STEP_COUNT):\n",
        "            if her_step_inds[i-1]:\n",
        "                her_step_inds[i] = 0\n",
        "\n",
        "#        assert 1 == HRL_STEP_COUNT or len(goals) - 1 == (len(goals) // HRL_STEP_COUNT) * HRL_STEP_COUNT\n",
        "\n",
        "        assert not sum(her_step_inds[-2:])\n",
        "\n",
        "        for j, (r, g, s, s2, n_g, n, u, step) in enumerate(zip(reversed(rewards), reversed(goals), reversed(states), reversed(states_1), reversed(n_goals), reversed(n_states), reversed(her_step_inds), reversed(n_steps))):\n",
        "            if not step:\n",
        "                continue\n",
        "\n",
        "            i = len(goals) - 1 - j\n",
        "            if i >= len(goals) - SENTINEL:\n",
        "                continue\n",
        "\n",
        "            her_active = her_step_inds[i+1]\n",
        "#            assert her_active or not her_step_inds[i+step]\n",
        "            if not her_active and her_step_inds[i+step]:\n",
        "                allowed_mask[i] = False\n",
        "\n",
        "            if not her_active and u:\n",
        "                gro = random.randint(1, 1 + (len(goals) - i - step - SENTINEL) // HRL_STEP_COUNT)\n",
        "                if random.random() < self.her_select_ratio:\n",
        "                    gro = 1\n",
        "\n",
        "            if her_active or u:\n",
        "                if 1 == gro:\n",
        "#                    assert i+1 == gid\n",
        "                    h_rewards[i] = (config.REWARD_DELTA + torch.zeros(1, 1)) * config.REWARD_MAGNITUDE\n",
        "                    x += 1\n",
        "                    hers.append(i)\n",
        "                else:\n",
        "                    h_rewards[i] = (config.REWARD_DELTA - torch.ones(1, 1)) * config.REWARD_MAGNITUDE\n",
        "                    z += 1\n",
        "                    others.append(i)\n",
        "\n",
        "#                if align(i) != align(step+i):\n",
        "#                    print(\"===>\", i, step, align(i), align(step+i), gro, align(step + i) + gro * config.HRL_STEP_COUNT)\n",
        "#                assert align(step + i) + gro * config.HRL_STEP_COUNT <= 100\n",
        "\n",
        "                hg = [align(i) + gro * config.HRL_STEP_COUNT, align(step + i) + gro * config.HRL_STEP_COUNT]\n",
        "                hs = [align(i), align(step+i)]\n",
        "\n",
        "                idxs.append(i)\n",
        "                her_goals.extend(hg)\n",
        "                her_states.extend(hs)\n",
        "\n",
        "            else:\n",
        "                others.append(i)\n",
        "\n",
        "        allowed = allowed_mask[idxs]\n",
        "        allowed_mask[...] = False # rest we dont know if good or not\n",
        "        allowed_mask[idxs] = allowed\n",
        "\n",
        "#        mask = np.zeros(len(n_steps))\n",
        "#        mask[idxs] = 1.\n",
        "#        if sum(her_step_inds): print(\"\\nHUH\", np.concatenate([np.asarray(n_steps).reshape(-1, 1), np.asarray(her_step_inds).reshape(-1, 1), mask.reshape(-1, 1)], 1))\n",
        "\n",
        "        if len(hers):\n",
        "\n",
        "            h_states[idxs, -config.CORE_ORIGINAL_GOAL_SIZE:] = states[her_goals[0::2], :config.CORE_ORIGINAL_GOAL_SIZE].clone()\n",
        "            h_n_states[idxs, -config.CORE_ORIGINAL_GOAL_SIZE:] = states[her_goals[1::2], :config.CORE_ORIGINAL_GOAL_SIZE].clone()\n",
        "\n",
        "            her_states_t = h_states[her_states].view(len(her_states), -1).clone()\n",
        "#            assert her_states_t[1::2, -config.CORE_ORIGINAL_GOAL_SIZE:].shape == states[her_goals[1::2], :config.CORE_ORIGINAL_GOAL_SIZE].clone().shape\n",
        "            her_states_t[1::2, -config.CORE_ORIGINAL_GOAL_SIZE:] = states[her_goals[1::2], :config.CORE_ORIGINAL_GOAL_SIZE].clone()\n",
        "\n",
        "            if config.TIMEFEAT:\n",
        "                if config.TF_LOW:\n",
        "                    if not config.NORMALIZE: # TODO this is just temporarerly\n",
        "                        pass#assert False\n",
        "                        #her_states_t[:, -1 - config.CORE_ORIGINAL_GOAL_SIZE] = 1.\n",
        "                else:\n",
        "                    assert False\n",
        "                    her_states_t[:, -1 - config.CORE_ORIGINAL_GOAL_SIZE] = (1. - (torch.tensor(her_states) /  (1.+config.HRL_HIGH_STEP)))# * .1\n",
        "\n",
        "            dist, _, _ = config.AGENT[1].exploit(\n",
        "                    h_states[her_goals, :CORE_GOAL_SIZE].view(len(her_goals), -1),\n",
        "                    her_states_t,\n",
        "                    torch.zeros(len(her_goals), 1), 0)\n",
        "            her_hers = dist.sample().view(len(her_goals), -1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            dist, _, _ = config.AGENT[0].exploit(\n",
        "                    her_hers,\n",
        "                    her_states_t,\n",
        "                    torch.zeros(len(her_goals), 1), 0)\n",
        "            bool_inds = (dist.log_prob(actions[her_states]).mean(1) < -1.)\n",
        "            hi = torch.tensor(her_goals)[bool_inds][::2]\n",
        "            if len(hi) * 3 >= len(idxs) * 2: print(\"DISBANDED {} vs {}\".format(len(hi), len(idxs)))\n",
        "            allowed_mask[hi] = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            h_goals[idxs] = her_hers[0::2].float()\n",
        "            h_n_goals[idxs] = her_hers[1::2].float()\n",
        "\n",
        "#        print(\"\\n ->>>>\", sum(allowed_mask))\n",
        "\n",
        "        return ( h_rewards, h_goals, h_states, h_n_goals, h_n_states, allowed_mask )"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0lyZUDMFP8M"
      },
      "source": [
        "from head import install_lowlevel\n",
        "from torch.distributions import Normal\n",
        "from utils.schedule import LinearSchedule\n",
        "\n",
        "class HighLevelCtrlTask:\n",
        "    def __init__(self, dock, prefix):\n",
        "        self.ll_ctrl = LowLevelCtrlTask(dock, prefix)\n",
        "        self.ll_env, self.ll_task = install_lowlevel(self.ll_ctrl, ReacherHRL)\n",
        "\n",
        "        self.lowlevel = None\n",
        "        \n",
        "        self.ls = LinearSchedule(.1, .8, config.HRL_HINDSIGHTACTION_HORIZON)\n",
        "        self.ctrl = None\n",
        "\n",
        "        #DEBUG\n",
        "        self.total = 0\n",
        "        self.probed = 0\n",
        "\n",
        "        self.goals = None\n",
        "        self.goods = None\n",
        "\n",
        "    def get_goals(self):\n",
        "        return self.goals\n",
        "\n",
        "    def _state(self, \n",
        "            einfo, base_states, rewards, actions, goods, pi,\n",
        "            learn_mode=False, reset=False, seed=None):\n",
        "\n",
        "        states = einfo.states.clone()\n",
        "\n",
        "        self.goods = [(g0 or g1) for g0, g1 in zip(self.goods, goods)] if goods is not None else [False for _ in range(len(states))]\n",
        "        \n",
        "        info = Info(\n",
        "                states,\n",
        "                rewards,\n",
        "                actions,\n",
        "# custom rewards shape                    \n",
        "                (einfo.rewards + config.REWARD_DELTA) * config.REWARD_MAGNITUDE,\n",
        "                einfo.dones,\n",
        "                self.goals,\n",
        "                goods,\n",
        "                pi,\n",
        "                )\n",
        "\n",
        "        self.info = info\n",
        "        return info\n",
        "\n",
        "    def _finish_ep(self):\n",
        "        if self.lowlevel is None:\n",
        "            return\n",
        "        # allow to learn only from eps where we moved\n",
        "        print(\"\\n ep selection --->\", sum(self.goods))\n",
        "        self.ll_ctrl.set_goods(self.goods)\n",
        "        for _ in self.lowlevel:\n",
        "            print(\"DO FINISH\")\n",
        "\n",
        "    def reset(self, agent, seed, learn_mode):\n",
        "        self.learn_mode = learn_mode\n",
        "        \n",
        "        timebudget.report(reset=True)\n",
        "        \n",
        "        print(\"\\nstats : \", self.probed, self.total, self.ls.c, (self.ls.get_ls() - 1.), learn_mode)\n",
        "        self.total = 0\n",
        "        self.probed = 0\n",
        "\n",
        "        self._finish_ep()\n",
        "\n",
        "        self.lowlevel = self.ll_env.step(\n",
        "            self.ll_task, seed, config.HRL_STEP_COUNT) if learn_mode else self.ll_env.evaluate(\n",
        "            self.ll_task, config.HRL_STEP_COUNT)\n",
        "\n",
        "        self.goals = self.ll_ctrl.internal_reset(agent, seed, learn_mode)\n",
        "\n",
        "        return self._state(\n",
        "            self.ll_ctrl.get_info(), \n",
        "            None, None, None, None, None,\n",
        "            learn_mode=learn_mode, reset=True, seed=seed)\n",
        "    \n",
        "    def step(self, pi):\n",
        "        a = pi[:, :pi.shape[1]//3].clone()\n",
        "\n",
        "        self.ll_ctrl.set_goals(a.clone())\n",
        "        base_states = self.ll_ctrl.get_info().states.clone()\n",
        "\n",
        "        (log_prob, _, _, _, _, ll_actions, _, good), acu_reward = next(self.lowlevel)\n",
        "        \n",
        "        next_states = self.ll_ctrl.get_info().states.clone()\n",
        "        \n",
        "        goods = select_exp(base_states, next_states)\n",
        "        actions = self.proximaly_close_actions(a, pi, base_states, next_states, goods)\n",
        "\n",
        "        self.einfo = self._state(\n",
        "            self.ll_ctrl.get_info(), \n",
        "            base_states,\n",
        "            acu_reward, actions, goods, \n",
        "            pi,\n",
        "            self.learn_mode, reset=False)\n",
        "\n",
        "        self.einfo.pi[:, actions.shape[-1]:actions.shape[-1]+ll_actions.shape[-1]*2] = torch.cat([ # TODO[ : KICK OFF\n",
        "            log_prob,#torch.ones_like(log_prob ),\n",
        "            ll_actions], 1)#torch.ones_like(actionsZ) ], 1)\n",
        "\n",
        "        return self.einfo\n",
        "    \n",
        "    def proximaly_close_actions(self, a, actions, base_states, next_states, goods):\n",
        "        self.total += len(a)\n",
        "        \n",
        "        if not self.learn_mode:\n",
        "            return a\n",
        "        \n",
        "        pi = Normal(actions[:, a.shape[1]: a.shape[1]*2], actions[:, a.shape[1]*2:])\n",
        "        og = self.ll_ctrl.optimal_goals(base_states, next_states)\n",
        "\n",
        "        baseline = pi.log_prob(a).mean(1) * (1. + 1. - self.ls.get_ls())\n",
        "\n",
        "        idx = pi.log_prob(og).mean(1) > baseline\n",
        "        if not sum(idx):\n",
        "            return a\n",
        "        \n",
        "        a[idx > 0] = og[idx > 0]\n",
        "        self.probed += sum(idx)\n",
        "\n",
        "        if sum(idx) * 2 > len(og) and sum(goods):\n",
        "            return a\n",
        "        \n",
        "        self.ls()    \n",
        "        return a"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "mS6swup7FP8X",
        "scrolled": false,
        "outputId": "ad479137-5cca-40fc-f9b6-3469d10d3881"
      },
      "source": [
        "timebudget.set_quiet()\n",
        "import time\n",
        "env_start = time.time()\n",
        "env_counter = 0\n",
        "\n",
        "import hashlib\n",
        "algo = hashlib.md5()\n",
        "algo.update(open(\"dlppoh/config.py\", \"rb\").read())\n",
        "import logging\n",
        "\n",
        "LOG_NAME = \"stat_\"+config.PREFIX+\"_\"+algo.hexdigest()+\"%i\"%time.time()\n",
        "def log_info():\n",
        "    return LOG_NAME\n",
        "\n",
        "logging.basicConfig(filename=LOG_NAME, level=logging.DEBUG)\n",
        "logging.info(open(\"dlppoh/config.py\", \"r\").read())\n",
        "def callback(bot, task, test_scores, learn_scores, seeds, total):\n",
        "    global env_start, env_counter\n",
        "    env_counter += 1\n",
        "    if test_scores is None:\n",
        "        return\n",
        "    msg = (\"\\n\\t [\", env_counter, \"] < %.2f\"%((time.time()-env_start) / 60), \"min > TEST ==> \", test_scores, \"exploring score:\", learn_scores.mean())\n",
        "    logging.info(msg)\n",
        "    print(*msg)\n",
        "    print(\"debug info ->\", LOG_NAME)\n",
        "    if env_counter > config.TOTAL_ROUNDS:\n",
        "        exit()\n",
        "        import os\n",
        "        os.system(\"sh looper.sh\")\n",
        "\n",
        "from head import install_highlevel\n",
        "#from dlhppo import HighLevelCtrlTask\n",
        "\n",
        "print(\"\\nSTART$log is in : \", log_info())\n",
        "\n",
        "KEYID = config.PREFIX+\"_hl\"\n",
        "high_level_task = HighLevelCtrlTask(\"mujoco_dock\", KEYID)\n",
        "\n",
        "env, task = install_highlevel(high_level_task, KEYID)\n",
        "scores = env.start(task, callback, True)\n",
        "\n",
        "print(\"\\nDONE$log is in : \", log_info())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-de4e6545f0e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0malgo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmd5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"config.py\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'config.py'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my1KUtx3FP8d"
      },
      "source": [
        "for _ in env.evaluate(task, None):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt968tU5FP8g"
      },
      "source": [
        "!cat stat__v2_1010_Reacher_OWNSTUFF_6e25cc3b03badf144c111d3ae287c6cd1624905322"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tWPC9qvFP8j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}