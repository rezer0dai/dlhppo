{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 458,
     "status": "ok",
     "timestamp": 1624585728025,
     "user": {
      "displayName": "Peter Hlavaty",
      "photoUrl": "",
      "userId": "07889617686998860112"
     },
     "user_tz": -120
    },
    "id": "enplmcsCp_ha"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1228,
     "status": "ok",
     "timestamp": 1624585729775,
     "user": {
      "displayName": "Peter Hlavaty",
      "photoUrl": "",
      "userId": "07889617686998860112"
     },
     "user_tz": -120
    },
    "id": "RX0fHSr3HI_6",
    "outputId": "261416af-abee-4d49-e21d-b224b5916ac4"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/rezer0dai/dlppoh\n",
    "!git clone https://github.com/fgolemo/gym-ergojr\n",
    "!git clone https://github.com/mahyaret/gym-panda\n",
    "\n",
    "!pip install -e gym-ergojr\n",
    "!pip install -e gym-panda\n",
    "!pip install timebudget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1624585729776,
     "user": {
      "displayName": "Peter Hlavaty",
      "photoUrl": "",
      "userId": "07889617686998860112"
     },
     "user_tz": -120
    },
    "id": "unEYPXkVJdGE"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"dlppoh\")\n",
    "sys.path.append(\"gym-ergojr\")\n",
    "sys.path.append(\"gym-panda\")\n",
    "sys.path.append(\"/content/dlppoh\")\n",
    "sys.path.append(\"/content/gym-ergojr\")\n",
    "sys.path.append(\"/content/gym-panda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 465,
     "status": "ok",
     "timestamp": 1624585730234,
     "user": {
      "displayName": "Peter Hlavaty",
      "photoUrl": "",
      "userId": "07889617686998860112"
     },
     "user_tz": -120
    },
    "id": "yrsg1IhfFP7p"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1624585730236,
     "user": {
      "displayName": "Peter Hlavaty",
      "photoUrl": "",
      "userId": "07889617686998860112"
     },
     "user_tz": -120
    },
    "id": "3pRLFULDFP75"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random, timebudget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1624585730237,
     "user": {
      "displayName": "Peter Hlavaty",
      "photoUrl": "",
      "userId": "07889617686998860112"
     },
     "user_tz": -120
    },
    "id": "_02ho3HYFP77"
   },
   "outputs": [],
   "source": [
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1624585730241,
     "user": {
      "displayName": "Peter Hlavaty",
      "photoUrl": "",
      "userId": "07889617686998860112"
     },
     "user_tz": -120
    },
    "id": "pucIFKuNFP8A"
   },
   "outputs": [],
   "source": [
    "class Info:\n",
    "    def __init__(self, states, rewards, actions, custom_rewards, dones, goals, goods, pi):\n",
    "        self.states = states\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "        self.custom_rewards = custom_rewards\n",
    "        self.dones = dones\n",
    "        self.goals = goals\n",
    "        self.goods = goods\n",
    "        self.pi = pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1624585730242,
     "user": {
      "displayName": "Peter Hlavaty",
      "photoUrl": "",
      "userId": "07889617686998860112"
     },
     "user_tz": -120
    },
    "id": "wv4i_24kFP8E"
   },
   "outputs": [],
   "source": [
    "ones = lambda *shape: torch.ones(*shape)\n",
    "zeros = lambda *shape: torch.zeros(*shape)\n",
    "\n",
    "tensor = lambda x, shape: torch.tensor(x).view(*shape)\n",
    "\n",
    "achieved_goals = lambda states: states[:, :config.CORE_ORIGINAL_GOAL_SIZE]\n",
    "\n",
    "MOVE_DIST = 3e-4 \n",
    "moved = lambda s1, s: MOVE_DIST < torch.norm(s1 - s)\n",
    "\n",
    "def select_exp(states_1, states):\n",
    "    if not config.SELECT_EXP:\n",
    "        return [True] * len(states)\n",
    "    return [moved(s1, s) for s1, s in zip(achieved_goals(states_1), achieved_goals(states))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1624585730243,
     "user": {
      "displayName": "Peter Hlavaty",
      "photoUrl": "",
      "userId": "07889617686998860112"
     },
     "user_tz": -120
    },
    "id": "7r7c2ctbFP8H"
   },
   "outputs": [],
   "source": [
    "from tasks.oaiproc import GymGroup, GymRender\n",
    "\n",
    "class LowLevelCtrlTask:\n",
    "    def __init__(self, dock, prefix):\n",
    "        self.ENV = GymGroup(config.TEST_ENVS, dock, config.TOTAL_ENV, prefix)\n",
    "        self.RENDER = GymRender(config.ENV_NAME, config.TOTAL_ENV)\n",
    "        \n",
    "        self.goals = None\n",
    "        self.info = None\n",
    "        self.set_goods(False)\n",
    "\n",
    "    def set_goods(self, goods):\n",
    "        self.goods = goods\n",
    "\n",
    "    def get_info(self):\n",
    "        return self.info\n",
    "        \n",
    "    def set_goals(self, goals):\n",
    "        self.goals = goals\n",
    "\n",
    "    def get_goals(self):\n",
    "        return self.goals\n",
    "\n",
    "    def optimal_goals(self, base_states, end_states):\n",
    "        base_states[:, -config.CORE_ORIGINAL_GOAL_SIZE:] = achieved_goals(self.info.states)\n",
    "        dist, _, _ = self.hl_agent.exploit(\n",
    "            end_states[:, :config.CORE_GOAL_SIZE],\n",
    "            base_states,\n",
    "            zeros(len(self.info.states), 1), 0)\n",
    "        return dist.sample()\n",
    "    \n",
    "    # time feature is only for critic\n",
    "    def append_time_feat(self, states):\n",
    "        self.n_steps += 1\n",
    "        if not config.TIMEFEAT:\n",
    "            return states\n",
    "        assert self.n_steps <  (1. + config.HRL_HIGH_STEP * config.HRL_STEP_COUNT)\n",
    "        tf = ones(states.shape[0], 1) - (self.n_steps /  (1. + config.HRL_HIGH_STEP * config.HRL_STEP_COUNT))\n",
    "        return torch.cat([states, tf], 1)\n",
    "\n",
    "    def _state(self, \n",
    "            einfo, actions, pi,\n",
    "            learn_mode=False, reset=False, seed=None):\n",
    "\n",
    "        states, goals, rewards, dones = einfo\n",
    "\n",
    "        states = self.append_time_feat(states)\n",
    "        \n",
    "        states = torch.cat([states, goals], 1)\n",
    "        if config.CORE_GOAL_SIZE != config.CORE_ORIGINAL_GOAL_SIZE:\n",
    "            goals = np.concatenate([goals, self.orig_pos], 1)\n",
    "\n",
    "        goods = self.goods if self.goods is not None else [False for _ in range(len(states))] \n",
    "        rewards = tensor(rewards, [-1, 1])\n",
    "        self.info = Info(\n",
    "                states,\n",
    "                rewards,\n",
    "                actions,\n",
    "# custom rewards here does not matter, all experience will be \"dreamed\" but based on true exp\n",
    "                rewards,\n",
    "                tensor(dones, [len(dones), -1]).float(),\n",
    "                self.goals,\n",
    "                goods,\n",
    "                pi,\n",
    "                )\n",
    "        \n",
    "        return self.info\n",
    "    \n",
    "    def reset(self, agent, seed, learn_mode):\n",
    "        return self.info\n",
    "\n",
    "    def internal_reset(self, agent, seed, learn_mode):\n",
    "        self.hl_agent = agent\n",
    "        self.n_steps = 0\n",
    "        self.learn_mode = learn_mode\n",
    "\n",
    "        if self.learn_mode:\n",
    "            einfo = self.ENV.reset(seed)\n",
    "        else:\n",
    "            einfo = self.RENDER.reset(seed)\n",
    "            \n",
    "        self.set_goods(None)\n",
    "    \n",
    "        self._state(\n",
    "            einfo, \n",
    "            None, None,\n",
    "            learn_mode, True, seed)\n",
    "        \n",
    "        return einfo[1]\n",
    "    \n",
    "    def step(self, pi):\n",
    "        if self.learn_mode:\n",
    "            einfo = self.ENV.step(\n",
    "                    pi[:, :pi.shape[1]//3].cpu().numpy(), ones(len(pi)))# if sum(self.goods) else None)\n",
    "        else:\n",
    "            einfo = self.RENDER.step(\n",
    "                    pi[:, :pi.shape[1]//3].cpu().numpy())\n",
    "\n",
    "        return self._state(einfo, pi[:, :config.ACTION_SIZE], pi, self.learn_mode, False)\n",
    "\n",
    "    def goal_met(self, _total_reward, _last_reward):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1624585730245,
     "user": {
      "displayName": "Peter Hlavaty",
      "photoUrl": "",
      "userId": "07889617686998860112"
     },
     "user_tz": -120
    },
    "id": "9sVml8zQFP8Q"
   },
   "outputs": [],
   "source": [
    "from config import *\n",
    "from timebudget import timebudget\n",
    "\n",
    "from utils.her import HER, CreditAssignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1624585730632,
     "user": {
      "displayName": "Peter Hlavaty",
      "photoUrl": "",
      "userId": "07889617686998860112"
     },
     "user_tz": -120
    },
    "id": "fRGnro3MFP8T"
   },
   "outputs": [],
   "source": [
    "COUNTER = [0, 0]\n",
    "\n",
    "SENTINEL = 2 # skip +1 last state dummy n_state it has, and last with valid n_state to use fast low level has +1 state we can throw when bit step_count ( 10 + ) and not waste too much experience\n",
    "class ReacherHRL(HER):\n",
    "    def __init__(self, cind, her_delay, gae, n_step, floating_step, gamma, gae_tau, her_select_ratio=.4, resampling=False, kstep_ir=False, clip=None):\n",
    "        super().__init__(cind, her_delay, gae, n_step, floating_step, gamma, gae_tau, her_select_ratio, resampling, kstep_ir, clip)\n",
    "\n",
    "    def _her_select_idx(self, n_steps):\n",
    "        hers = [ i if random.random() < .5 else 0 for i, s in enumerate(n_steps[:-SENTINEL]) ]\n",
    "        return hers + [0 for _ in range(SENTINEL)]\n",
    "\n",
    "    @timebudget\n",
    "    def update_goal(self, rewards, goals, states, states_1, n_goals, n_states, actions, her_step_inds, n_steps, allowed_mask):\n",
    "        global COUNTER\n",
    "\n",
    "        align = lambda x: x//config.HRL_STEP_COUNT*config.HRL_STEP_COUNT\n",
    "\n",
    "        MAX_HER_STEP = 1\n",
    "\n",
    "        h_goals = goals.clone()\n",
    "        h_n_goals = n_goals.clone()\n",
    "        h_rewards = rewards.clone()\n",
    "\n",
    "        h_states = states.clone()\n",
    "        h_n_states = n_states.clone()\n",
    "\n",
    "        assert not allowed_mask[-1] # last goal will be not used!!\n",
    "        allowed_mask[-2] = False # last state_1 should not be used too\n",
    "\n",
    "        idxs = []\n",
    "        her_goals = []\n",
    "        her_states = []\n",
    "\n",
    "        x = 0\n",
    "        z = 0\n",
    "\n",
    "        hers = []\n",
    "        others = []\n",
    "\n",
    "        COUNTER[sum(her_step_inds) > 0] += 1\n",
    "        for i in range(HRL_STEP_COUNT, len(goals)-1, HRL_STEP_COUNT):\n",
    "            if her_step_inds[i-1]:\n",
    "                her_step_inds[i] = 0\n",
    "\n",
    "#        assert 1 == HRL_STEP_COUNT or len(goals) - 1 == (len(goals) // HRL_STEP_COUNT) * HRL_STEP_COUNT\n",
    "\n",
    "        assert not sum(her_step_inds[-2:])\n",
    "\n",
    "        for j, (r, g, s, s2, n_g, n, u, step) in enumerate(zip(reversed(rewards), reversed(goals), reversed(states), reversed(states_1), reversed(n_goals), reversed(n_states), reversed(her_step_inds), reversed(n_steps))):\n",
    "            if not step:\n",
    "                continue\n",
    "\n",
    "            i = len(goals) - 1 - j\n",
    "            if i >= len(goals) - SENTINEL:\n",
    "                continue\n",
    "\n",
    "            her_active = her_step_inds[i+1]\n",
    "#            assert her_active or not her_step_inds[i+step]\n",
    "            if not her_active and her_step_inds[i+step]:\n",
    "                allowed_mask[i] = False\n",
    "\n",
    "            if not her_active and u:\n",
    "                gro = random.randint(1, 1 + (len(goals) - i - step - SENTINEL) // HRL_STEP_COUNT)\n",
    "                if random.random() < self.her_select_ratio:\n",
    "                    gro = 1\n",
    "\n",
    "            if her_active or u:\n",
    "                if 1 == gro:\n",
    "#                    assert i+1 == gid\n",
    "                    h_rewards[i] = (config.REWARD_DELTA + torch.zeros(1, 1)) * config.REWARD_MAGNITUDE\n",
    "                    x += 1\n",
    "                    hers.append(i)\n",
    "                else:\n",
    "                    h_rewards[i] = (config.REWARD_DELTA - torch.ones(1, 1)) * config.REWARD_MAGNITUDE\n",
    "                    z += 1\n",
    "                    others.append(i)\n",
    "\n",
    "#                if align(i) != align(step+i):\n",
    "#                    print(\"===>\", i, step, align(i), align(step+i), gro, align(step + i) + gro * config.HRL_STEP_COUNT)\n",
    "#                assert align(step + i) + gro * config.HRL_STEP_COUNT <= 100\n",
    "\n",
    "                hg = [align(i) + gro * config.HRL_STEP_COUNT, align(step + i) + gro * config.HRL_STEP_COUNT]\n",
    "                hs = [align(i), align(step+i)]\n",
    "\n",
    "                idxs.append(i)\n",
    "                her_goals.extend(hg)\n",
    "                her_states.extend(hs)\n",
    "\n",
    "            else:\n",
    "                others.append(i)\n",
    "\n",
    "        allowed = allowed_mask[idxs]\n",
    "        allowed_mask[...] = False # rest we dont know if good or not\n",
    "        allowed_mask[idxs] = allowed\n",
    "\n",
    "#        mask = np.zeros(len(n_steps))\n",
    "#        mask[idxs] = 1.\n",
    "#        if sum(her_step_inds): print(\"\\nHUH\", np.concatenate([np.asarray(n_steps).reshape(-1, 1), np.asarray(her_step_inds).reshape(-1, 1), mask.reshape(-1, 1)], 1))\n",
    "\n",
    "        if len(hers):\n",
    "\n",
    "            h_states[idxs, -config.CORE_ORIGINAL_GOAL_SIZE:] = states[her_goals[0::2], :config.CORE_ORIGINAL_GOAL_SIZE].clone()\n",
    "            h_n_states[idxs, -config.CORE_ORIGINAL_GOAL_SIZE:] = states[her_goals[1::2], :config.CORE_ORIGINAL_GOAL_SIZE].clone()\n",
    "\n",
    "            her_states_t = h_states[her_states].view(len(her_states), -1).clone()\n",
    "#            assert her_states_t[1::2, -config.CORE_ORIGINAL_GOAL_SIZE:].shape == states[her_goals[1::2], :config.CORE_ORIGINAL_GOAL_SIZE].clone().shape\n",
    "            her_states_t[1::2, -config.CORE_ORIGINAL_GOAL_SIZE:] = states[her_goals[1::2], :config.CORE_ORIGINAL_GOAL_SIZE].clone()\n",
    "\n",
    "            if config.TIMEFEAT:\n",
    "                if config.TF_LOW:\n",
    "                    if not config.NORMALIZE: # TODO this is just temporarerly\n",
    "                        pass#assert False\n",
    "                        #her_states_t[:, -1 - config.CORE_ORIGINAL_GOAL_SIZE] = 1.\n",
    "                else:\n",
    "                    assert False\n",
    "                    her_states_t[:, -1 - config.CORE_ORIGINAL_GOAL_SIZE] = (1. - (torch.tensor(her_states) /  (1.+config.HRL_HIGH_STEP)))# * .1\n",
    "\n",
    "            dist, _, _ = config.AGENT[1].exploit(\n",
    "                    h_states[her_goals, :CORE_GOAL_SIZE].view(len(her_goals), -1),\n",
    "                    her_states_t,\n",
    "                    torch.zeros(len(her_goals), 1), 0)\n",
    "            her_hers = dist.sample().view(len(her_goals), -1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            dist, _, _ = config.AGENT[0].exploit(\n",
    "                    her_hers,\n",
    "                    her_states_t,\n",
    "                    torch.zeros(len(her_goals), 1), 0)\n",
    "            bool_inds = (dist.log_prob(actions[her_states]).mean(1) < -1.)\n",
    "            hi = torch.tensor(her_goals)[bool_inds][::2]\n",
    "            if len(hi) * 3 >= len(idxs) * 2: print(\"DISBANDED {} vs {}\".format(len(hi), len(idxs)))\n",
    "            allowed_mask[hi] = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            h_goals[idxs] = her_hers[0::2].float()\n",
    "            h_n_goals[idxs] = her_hers[1::2].float()\n",
    "\n",
    "#        print(\"\\n ->>>>\", sum(allowed_mask))\n",
    "\n",
    "        return ( h_rewards, h_goals, h_states, h_n_goals, h_n_states, allowed_mask )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 492,
     "status": "ok",
     "timestamp": 1624585731109,
     "user": {
      "displayName": "Peter Hlavaty",
      "photoUrl": "",
      "userId": "07889617686998860112"
     },
     "user_tz": -120
    },
    "id": "S0lyZUDMFP8M"
   },
   "outputs": [],
   "source": [
    "from head import install_lowlevel\n",
    "from torch.distributions import Normal\n",
    "from utils.schedule import LinearSchedule\n",
    "\n",
    "class HighLevelCtrlTask:\n",
    "    def __init__(self, dock, prefix):\n",
    "        self.ll_ctrl = LowLevelCtrlTask(dock, prefix)\n",
    "        self.ll_env, self.ll_task = install_lowlevel(self.ll_ctrl, ReacherHRL)\n",
    "\n",
    "        self.lowlevel = None\n",
    "        \n",
    "        self.ls = LinearSchedule(.1, .8, config.HRL_HINDSIGHTACTION_HORIZON)\n",
    "        self.ctrl = None\n",
    "\n",
    "        #DEBUG\n",
    "        self.total = 0\n",
    "        self.probed = 0\n",
    "\n",
    "        self.goals = None\n",
    "        self.goods = None\n",
    "\n",
    "    def get_goals(self):\n",
    "        return self.goals\n",
    "\n",
    "    def _state(self, \n",
    "            einfo, base_states, rewards, actions, goods, pi,\n",
    "            learn_mode=False, reset=False, seed=None):\n",
    "\n",
    "        states = einfo.states.clone()\n",
    "\n",
    "        self.goods = [(g0 or g1) for g0, g1 in zip(self.goods, goods)] if goods is not None else [False for _ in range(len(states))]\n",
    "        \n",
    "        info = Info(\n",
    "                states,\n",
    "                rewards,\n",
    "                actions,\n",
    "# custom rewards shape                    \n",
    "                (einfo.rewards + config.REWARD_DELTA) * config.REWARD_MAGNITUDE,\n",
    "                einfo.dones,\n",
    "                self.goals,\n",
    "                goods,\n",
    "                pi,\n",
    "                )\n",
    "\n",
    "        self.info = info\n",
    "        return info\n",
    "\n",
    "    def _finish_ep(self):\n",
    "        if self.lowlevel is None:\n",
    "            return\n",
    "        # allow to learn only from eps where we moved\n",
    "        print(\"\\n ep selection --->\", sum(self.goods))\n",
    "        self.ll_ctrl.set_goods(self.goods)\n",
    "        for _ in self.lowlevel:\n",
    "            print(\"DO FINISH\")\n",
    "\n",
    "    def reset(self, agent, seed, learn_mode):\n",
    "        self.learn_mode = learn_mode\n",
    "        \n",
    "        timebudget.report(reset=True)\n",
    "        \n",
    "        print(\"\\nstats : \", self.probed, self.total, self.ls.c, (self.ls.get_ls() - 1.), learn_mode)\n",
    "        self.total = 0\n",
    "        self.probed = 0\n",
    "\n",
    "        self._finish_ep()\n",
    "\n",
    "        self.lowlevel = self.ll_env.step(\n",
    "            self.ll_task, seed, config.HRL_STEP_COUNT) if learn_mode else self.ll_env.evaluate(\n",
    "            self.ll_task, config.HRL_STEP_COUNT)\n",
    "\n",
    "        self.goals = self.ll_ctrl.internal_reset(agent, seed, learn_mode)\n",
    "\n",
    "        return self._state(\n",
    "            self.ll_ctrl.get_info(), \n",
    "            None, None, None, None, None,\n",
    "            learn_mode=learn_mode, reset=True, seed=seed)\n",
    "    \n",
    "    def step(self, pi):\n",
    "        a = pi[:, :pi.shape[1]//3].clone()\n",
    "\n",
    "        self.ll_ctrl.set_goals(a.clone())\n",
    "        base_states = self.ll_ctrl.get_info().states.clone()\n",
    "\n",
    "        (log_prob, _, _, _, _, ll_actions, _, good), acu_reward = next(self.lowlevel)\n",
    "        \n",
    "        next_states = self.ll_ctrl.get_info().states.clone()\n",
    "        \n",
    "        goods = select_exp(base_states, next_states)\n",
    "        actions = self.proximaly_close_actions(a, pi, base_states, next_states, goods)\n",
    "\n",
    "        self.einfo = self._state(\n",
    "            self.ll_ctrl.get_info(), \n",
    "            base_states,\n",
    "            acu_reward, actions, goods, \n",
    "            pi,\n",
    "            self.learn_mode, reset=False)\n",
    "\n",
    "        self.einfo.pi[:, actions.shape[-1]:actions.shape[-1]+ll_actions.shape[-1]*2] = torch.cat([ # TODO[ : KICK OFF\n",
    "            log_prob,#torch.ones_like(log_prob ),\n",
    "            ll_actions], 1)#torch.ones_like(actionsZ) ], 1)\n",
    "\n",
    "        return self.einfo\n",
    "    \n",
    "    def proximaly_close_actions(self, a, actions, base_states, next_states, goods):\n",
    "        self.total += len(a)\n",
    "        \n",
    "        if not self.learn_mode:\n",
    "            return a\n",
    "        \n",
    "        pi = Normal(actions[:, a.shape[1]: a.shape[1]*2], actions[:, a.shape[1]*2:])\n",
    "        og = self.ll_ctrl.optimal_goals(base_states, next_states)\n",
    "\n",
    "        baseline = pi.log_prob(a).mean(1) * (1. + 1. - self.ls.get_ls())\n",
    "\n",
    "        idx = pi.log_prob(og).mean(1) > baseline\n",
    "        if not sum(idx):\n",
    "            return a\n",
    "        \n",
    "        a[idx > 0] = og[idx > 0]\n",
    "        self.probed += sum(idx)\n",
    "\n",
    "        if sum(idx) * 2 > len(og) and sum(goods):\n",
    "            return a\n",
    "        \n",
    "        self.ls()    \n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mS6swup7FP8X",
    "outputId": "363fab3d-baf9-4268-c901-9c60006dddbc",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "timebudget.set_quiet()\n",
    "import time\n",
    "env_start = time.time()\n",
    "env_counter = 0\n",
    "\n",
    "import hashlib\n",
    "algo = hashlib.md5()\n",
    "algo.update(open(\"config.py\", \"rb\").read())\n",
    "import logging\n",
    "\n",
    "LOG_NAME = \"stat_\"+config.PREFIX+\"_\"+algo.hexdigest()+\"%i\"%time.time()\n",
    "def log_info():\n",
    "    return LOG_NAME\n",
    "\n",
    "logging.basicConfig(filename=LOG_NAME, level=logging.DEBUG)\n",
    "logging.info(open(\"config.py\", \"r\").read())\n",
    "def callback(bot, task, test_scores, learn_scores, seeds, total):\n",
    "    global env_start, env_counter\n",
    "    env_counter += 1\n",
    "    if test_scores is None:\n",
    "        return\n",
    "    msg = (\"\\n\\t [\", env_counter, \"] < %.2f\"%((time.time()-env_start) / 60), \"min > TEST ==> \", test_scores, \"exploring score:\", learn_scores.mean())\n",
    "    logging.info(msg)\n",
    "    print(*msg)\n",
    "    print(\"debug info ->\", LOG_NAME)\n",
    "    if env_counter > config.TOTAL_ROUNDS:\n",
    "        exit()\n",
    "        import os\n",
    "        os.system(\"sh looper.sh\")\n",
    "\n",
    "from head import install_highlevel\n",
    "#from dlhppo import HighLevelCtrlTask\n",
    "\n",
    "print(\"\\nSTART$log is in : \", log_info())\n",
    "\n",
    "KEYID = config.PREFIX+\"_hl\"\n",
    "high_level_task = HighLevelCtrlTask(\"mujoco_dock\", KEYID)\n",
    "\n",
    "env, task = install_highlevel(high_level_task, KEYID)\n",
    "scores = env.start(task, callback)\n",
    "\n",
    "print(\"\\nDONE$log is in : \", log_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "my1KUtx3FP8d"
   },
   "outputs": [],
   "source": [
    "for _ in env.evaluate(task, None):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pt968tU5FP8g"
   },
   "outputs": [],
   "source": [
    "!cat stat__v2_1010_Reacher_OWNSTUFF_6e25cc3b03badf144c111d3ae287c6cd1624905322"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tWPC9qvFP8j"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dlppoh.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/rezer0dai/dlppoh/blob/master/dlppoh.ipynb",
     "timestamp": 1624563142378
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
