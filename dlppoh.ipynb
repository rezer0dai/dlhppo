{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dlppoh.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "enplmcsCp_ha"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX0fHSr3HI_6"
      },
      "source": [
        "!git clone https://github.com/rezer0dai/dlppoh\n",
        "!git clone https://github.com/fgolemo/gym-ergojr\n",
        "!git clone https://github.com/qgallouedec/panda-gym\n",
        "\n",
        "!pip install -e gym-ergojr\n",
        "!pip install -e panda-gym\n",
        "!pip install timebudget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unEYPXkVJdGE"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"dlppoh\")\n",
        "sys.path.append(\"gym-ergojr\")\n",
        "sys.path.append(\"panda-gym\")\n",
        "sys.path.append(\"/content/dlppoh\")\n",
        "sys.path.append(\"/content/gym-ergojr\")\n",
        "sys.path.append(\"/content/panda-gym\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrsg1IhfFP7p"
      },
      "source": [
        "import torch\n",
        "torch.set_default_dtype(torch.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pRLFULDFP75"
      },
      "source": [
        "import numpy as np\n",
        "import random, timebudget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_02ho3HYFP77"
      },
      "source": [
        "import config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pucIFKuNFP8A"
      },
      "source": [
        "class Info:\n",
        "    def __init__(self, states, rewards, actions, custom_rewards, dones, goals, goods, pi):\n",
        "        self.states = states\n",
        "        self.rewards = rewards\n",
        "        self.actions = actions\n",
        "        self.custom_rewards = custom_rewards\n",
        "        self.dones = dones\n",
        "        self.goals = goals\n",
        "        self.goods = goods\n",
        "        self.pi = pi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wv4i_24kFP8E"
      },
      "source": [
        "ones = lambda *shape: torch.ones(*shape)\n",
        "zeros = lambda *shape: torch.zeros(*shape)\n",
        "\n",
        "tensor = lambda x, shape: torch.tensor(x).view(*shape)\n",
        "\n",
        "achieved_goals = lambda states: states[:, :config.CORE_ORIGINAL_GOAL_SIZE]\n",
        "\n",
        "MOVE_DIST = 3e-4 \n",
        "moved = lambda s1, s: MOVE_DIST < torch.norm(s1 - s)\n",
        "\n",
        "def select_exp(states_1, states):\n",
        "    if not config.SELECT_EXP:\n",
        "        return [True] * len(states)\n",
        "    return [moved(s1, s) for s1, s in zip(achieved_goals(states_1), achieved_goals(states))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r7c2ctbFP8H"
      },
      "source": [
        "from tasks.oaiproc import GymGroup, GymRender\n",
        "\n",
        "class LowLevelCtrlTask:\n",
        "    def __init__(self, dock, prefix):\n",
        "        self.ENV = GymGroup(config.TEST_ENVS, dock, config.TOTAL_ENV, prefix)\n",
        "        self.RENDER = GymRender(config.ENV_NAME, config.TOTAL_ENV)\n",
        "        \n",
        "        self.goals = None\n",
        "        self.info = None\n",
        "        self.set_goods(False)\n",
        "\n",
        "    def set_goods(self, goods):\n",
        "        self.goods = goods\n",
        "\n",
        "    def get_info(self):\n",
        "        return self.info\n",
        "        \n",
        "    def set_goals(self, goals):\n",
        "        self.goals = goals\n",
        "\n",
        "    def get_goals(self):\n",
        "        return self.goals\n",
        "\n",
        "    def optimal_goals(self, base_states, end_states):\n",
        "        base_states[:, -config.CORE_ORIGINAL_GOAL_SIZE:] = achieved_goals(self.info.states)\n",
        "        dist, _, _ = self.hl_agent.exploit(\n",
        "            end_states[:, :config.CORE_GOAL_SIZE],\n",
        "            base_states,\n",
        "            zeros(len(self.info.states), 1), 0)\n",
        "        return dist.sample()\n",
        "    \n",
        "    # time feature is only for critic\n",
        "    def append_time_feat(self, states):\n",
        "        self.n_steps += 1\n",
        "        if not config.TIMEFEAT:\n",
        "            return states\n",
        "        assert self.n_steps <  (1. + config.HRL_HIGH_STEP * config.HRL_STEP_COUNT)\n",
        "        tf = ones(states.shape[0], 1) - (self.n_steps /  (1. + config.HRL_HIGH_STEP * config.HRL_STEP_COUNT))\n",
        "        return torch.cat([states, tf], 1)\n",
        "\n",
        "    def _state(self, \n",
        "            einfo, actions, pi,\n",
        "            learn_mode=False, reset=False, seed=None):\n",
        "\n",
        "        states, goals, rewards, dones = einfo\n",
        "\n",
        "        states = self.append_time_feat(states)\n",
        "        \n",
        "        states = torch.cat([states, goals], 1)\n",
        "        if config.CORE_GOAL_SIZE != config.CORE_ORIGINAL_GOAL_SIZE:\n",
        "            goals = np.concatenate([goals, self.orig_pos], 1)\n",
        "\n",
        "        goods = self.goods if self.goods is not None else [False for _ in range(len(states))] \n",
        "        rewards = tensor(rewards, [-1, 1])\n",
        "        self.info = Info(\n",
        "                states,\n",
        "                rewards,\n",
        "                actions,\n",
        "# custom rewards here does not matter, all experience will be \"dreamed\" but based on true exp\n",
        "                rewards,\n",
        "                tensor(dones, [len(dones), -1]).float(),\n",
        "                self.goals,\n",
        "                goods,\n",
        "                pi,\n",
        "                )\n",
        "        \n",
        "        return self.info\n",
        "    \n",
        "    def reset(self, agent, seed, learn_mode):\n",
        "        return self.info\n",
        "\n",
        "    def internal_reset(self, agent, seed, learn_mode):\n",
        "        self.hl_agent = agent\n",
        "        self.n_steps = 0\n",
        "        self.learn_mode = learn_mode\n",
        "\n",
        "        if self.learn_mode:\n",
        "            einfo = self.ENV.reset(seed)\n",
        "        else:\n",
        "            einfo = self.RENDER.reset(seed)\n",
        "            \n",
        "        self.set_goods(None)\n",
        "    \n",
        "        self._state(\n",
        "            einfo, \n",
        "            None, None,\n",
        "            learn_mode, True, seed)\n",
        "        \n",
        "        return einfo[1]\n",
        "    \n",
        "    def step(self, pi):\n",
        "        if self.learn_mode:\n",
        "            einfo = self.ENV.step(\n",
        "                    pi[:, :pi.shape[1]//3].cpu().numpy(), ones(len(pi)))# if sum(self.goods) else None)\n",
        "        else:\n",
        "            einfo = self.RENDER.step(\n",
        "                    pi[:, :pi.shape[1]//3].cpu().numpy())\n",
        "\n",
        "        return self._state(einfo, pi[:, :config.ACTION_SIZE], pi, self.learn_mode, False)\n",
        "\n",
        "    def goal_met(self, _total_reward, _last_reward):\n",
        "        return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sVml8zQFP8Q"
      },
      "source": [
        "from config import *\n",
        "from timebudget import timebudget\n",
        "\n",
        "from utils.her import HER, CreditAssignment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRGnro3MFP8T"
      },
      "source": [
        "COUNTER = [0, 0]\n",
        "\n",
        "SENTINEL = 2 # skip +1 last state dummy n_state it has, and last with valid n_state to use fast low level has +1 state we can throw when bit step_count ( 10 + ) and not waste too much experience\n",
        "class ReacherHRL(HER):\n",
        "    def __init__(self, cind, her_delay, gae, n_step, floating_step, gamma, gae_tau, her_select_ratio=.4, resampling=False, kstep_ir=False, clip=None):\n",
        "        super().__init__(cind, her_delay, gae, n_step, floating_step, gamma, gae_tau, her_select_ratio, resampling, kstep_ir, clip)\n",
        "\n",
        "    def _her_select_idx(self, n_steps):\n",
        "        hers = [ i if random.random() < .5 else 0 for i, s in enumerate(n_steps[:-SENTINEL]) ]\n",
        "        return hers + [0 for _ in range(SENTINEL)]\n",
        "\n",
        "    @timebudget\n",
        "    def update_goal(self, rewards, goals, states, states_1, n_goals, n_states, actions, her_step_inds, n_steps, allowed_mask):\n",
        "        global COUNTER\n",
        "\n",
        "        align = lambda x: x//config.HRL_STEP_COUNT*config.HRL_STEP_COUNT\n",
        "\n",
        "        MAX_HER_STEP = 1\n",
        "\n",
        "        h_goals = goals.clone()\n",
        "        h_n_goals = n_goals.clone()\n",
        "        h_rewards = rewards.clone()\n",
        "\n",
        "        h_states = states.clone()\n",
        "        h_n_states = n_states.clone()\n",
        "\n",
        "        assert not allowed_mask[-1] # last goal will be not used!!\n",
        "        allowed_mask[-2] = False # last state_1 should not be used too\n",
        "\n",
        "        idxs = []\n",
        "        her_goals = []\n",
        "        her_states = []\n",
        "\n",
        "        x = 0\n",
        "        z = 0\n",
        "\n",
        "        hers = []\n",
        "        others = []\n",
        "\n",
        "        COUNTER[sum(her_step_inds) > 0] += 1\n",
        "        for i in range(HRL_STEP_COUNT, len(goals)-1, HRL_STEP_COUNT):\n",
        "            if her_step_inds[i-1]:\n",
        "                her_step_inds[i] = 0\n",
        "\n",
        "#        assert 1 == HRL_STEP_COUNT or len(goals) - 1 == (len(goals) // HRL_STEP_COUNT) * HRL_STEP_COUNT\n",
        "\n",
        "        assert not sum(her_step_inds[-2:])\n",
        "\n",
        "        for j, (r, g, s, s2, n_g, n, u, step) in enumerate(zip(reversed(rewards), reversed(goals), reversed(states), reversed(states_1), reversed(n_goals), reversed(n_states), reversed(her_step_inds), reversed(n_steps))):\n",
        "            if not step:\n",
        "                continue\n",
        "\n",
        "            i = len(goals) - 1 - j\n",
        "            if i >= len(goals) - SENTINEL:\n",
        "                continue\n",
        "\n",
        "            her_active = her_step_inds[i+1]\n",
        "#            assert her_active or not her_step_inds[i+step]\n",
        "            if not her_active and her_step_inds[i+step]:\n",
        "                allowed_mask[i] = False\n",
        "\n",
        "            if not her_active and u:\n",
        "                gro = random.randint(1, 1 + (len(goals) - i - step - SENTINEL) // HRL_STEP_COUNT)\n",
        "                if random.random() < self.her_select_ratio:\n",
        "                    gro = 1\n",
        "\n",
        "            if her_active or u:\n",
        "                if 1 == gro:\n",
        "#                    assert i+1 == gid\n",
        "                    h_rewards[i] = (config.REWARD_DELTA + torch.zeros(1, 1)) * config.REWARD_MAGNITUDE\n",
        "                    x += 1\n",
        "                    hers.append(i)\n",
        "                else:\n",
        "                    h_rewards[i] = (config.REWARD_DELTA - torch.ones(1, 1)) * config.REWARD_MAGNITUDE\n",
        "                    z += 1\n",
        "                    others.append(i)\n",
        "\n",
        "#                if align(i) != align(step+i):\n",
        "#                    print(\"===>\", i, step, align(i), align(step+i), gro, align(step + i) + gro * config.HRL_STEP_COUNT)\n",
        "#                assert align(step + i) + gro * config.HRL_STEP_COUNT <= 100\n",
        "\n",
        "                hg = [align(i) + gro * config.HRL_STEP_COUNT, align(step + i) + gro * config.HRL_STEP_COUNT]\n",
        "                hs = [align(i), align(step+i)]\n",
        "\n",
        "                idxs.append(i)\n",
        "                her_goals.extend(hg)\n",
        "                her_states.extend(hs)\n",
        "\n",
        "            else:\n",
        "                others.append(i)\n",
        "\n",
        "        allowed = allowed_mask[idxs]\n",
        "        allowed_mask[...] = False # rest we dont know if good or not\n",
        "        allowed_mask[idxs] = allowed\n",
        "\n",
        "#        mask = np.zeros(len(n_steps))\n",
        "#        mask[idxs] = 1.\n",
        "#        if sum(her_step_inds): print(\"\\nHUH\", np.concatenate([np.asarray(n_steps).reshape(-1, 1), np.asarray(her_step_inds).reshape(-1, 1), mask.reshape(-1, 1)], 1))\n",
        "\n",
        "        if len(hers):\n",
        "\n",
        "            h_states[idxs, -config.CORE_ORIGINAL_GOAL_SIZE:] = states[her_goals[0::2], :config.CORE_ORIGINAL_GOAL_SIZE].clone()\n",
        "            h_n_states[idxs, -config.CORE_ORIGINAL_GOAL_SIZE:] = states[her_goals[1::2], :config.CORE_ORIGINAL_GOAL_SIZE].clone()\n",
        "\n",
        "            her_states_t = h_states[her_states].view(len(her_states), -1).clone()\n",
        "#            assert her_states_t[1::2, -config.CORE_ORIGINAL_GOAL_SIZE:].shape == states[her_goals[1::2], :config.CORE_ORIGINAL_GOAL_SIZE].clone().shape\n",
        "            her_states_t[1::2, -config.CORE_ORIGINAL_GOAL_SIZE:] = states[her_goals[1::2], :config.CORE_ORIGINAL_GOAL_SIZE].clone()\n",
        "\n",
        "            if config.TIMEFEAT:\n",
        "                if config.TF_LOW:\n",
        "                    if not config.NORMALIZE: # TODO this is just temporarerly\n",
        "                        pass#assert False\n",
        "                        #her_states_t[:, -1 - config.CORE_ORIGINAL_GOAL_SIZE] = 1.\n",
        "                else:\n",
        "                    assert False\n",
        "                    her_states_t[:, -1 - config.CORE_ORIGINAL_GOAL_SIZE] = (1. - (torch.tensor(her_states) /  (1.+config.HRL_HIGH_STEP)))# * .1\n",
        "\n",
        "            dist, _, _ = config.AGENT[1].exploit(\n",
        "                    h_states[her_goals, :CORE_GOAL_SIZE].view(len(her_goals), -1),\n",
        "                    her_states_t,\n",
        "                    torch.zeros(len(her_goals), 1), 0)\n",
        "            her_hers = dist.sample().view(len(her_goals), -1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            dist, _, _ = config.AGENT[0].exploit(\n",
        "                    her_hers,\n",
        "                    her_states_t,\n",
        "                    torch.zeros(len(her_goals), 1), 0)\n",
        "            bool_inds = (dist.log_prob(actions[her_states]).mean(1) < -1.)\n",
        "            hi = torch.tensor(her_goals)[bool_inds][::2]\n",
        "            if len(hi) * 3 >= len(idxs) * 2: print(\"DISBANDED {} vs {}\".format(len(hi), len(idxs)))\n",
        "            allowed_mask[hi] = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            h_goals[idxs] = her_hers[0::2].float()\n",
        "            h_n_goals[idxs] = her_hers[1::2].float()\n",
        "\n",
        "#        print(\"\\n ->>>>\", sum(allowed_mask))\n",
        "\n",
        "        return ( h_rewards, h_goals, h_states, h_n_goals, h_n_states, allowed_mask )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0lyZUDMFP8M"
      },
      "source": [
        "from head import install_lowlevel\n",
        "from torch.distributions import Normal\n",
        "from utils.schedule import LinearSchedule\n",
        "\n",
        "class HighLevelCtrlTask:\n",
        "    def __init__(self, dock, prefix):\n",
        "        self.ll_ctrl = LowLevelCtrlTask(dock, prefix)\n",
        "        self.ll_env, self.ll_task = install_lowlevel(self.ll_ctrl, ReacherHRL)\n",
        "\n",
        "        self.lowlevel = None\n",
        "        \n",
        "        self.ls = LinearSchedule(.1, .8, config.HRL_HINDSIGHTACTION_HORIZON)\n",
        "        self.ctrl = None\n",
        "\n",
        "        #DEBUG\n",
        "        self.total = 0\n",
        "        self.probed = 0\n",
        "\n",
        "        self.goals = None\n",
        "        self.goods = None\n",
        "\n",
        "    def get_goals(self):\n",
        "        return self.goals\n",
        "\n",
        "    def _state(self, \n",
        "            einfo, base_states, rewards, actions, goods, pi,\n",
        "            learn_mode=False, reset=False, seed=None):\n",
        "\n",
        "        states = einfo.states.clone()\n",
        "\n",
        "        self.goods = [(g0 or g1) for g0, g1 in zip(self.goods, goods)] if goods is not None else [False for _ in range(len(states))]\n",
        "        \n",
        "        info = Info(\n",
        "                states,\n",
        "                rewards,\n",
        "                actions,\n",
        "# custom rewards shape                    \n",
        "                (einfo.rewards + config.REWARD_DELTA) * config.REWARD_MAGNITUDE,\n",
        "                einfo.dones,\n",
        "                self.goals,\n",
        "                goods,\n",
        "                pi,\n",
        "                )\n",
        "\n",
        "        self.info = info\n",
        "        return info\n",
        "\n",
        "    def _finish_ep(self):\n",
        "        if self.lowlevel is None:\n",
        "            return\n",
        "        # allow to learn only from eps where we moved\n",
        "        print(\"\\n ep selection --->\", sum(self.goods))\n",
        "        self.ll_ctrl.set_goods(self.goods)\n",
        "        for _ in self.lowlevel:\n",
        "            print(\"DO FINISH\")\n",
        "\n",
        "    def reset(self, agent, seed, learn_mode):\n",
        "        self.learn_mode = learn_mode\n",
        "        \n",
        "        timebudget.report(reset=True)\n",
        "        \n",
        "        print(\"\\nstats : \", self.probed, self.total, self.ls.c, (self.ls.get_ls() - 1.), learn_mode)\n",
        "        self.total = 0\n",
        "        self.probed = 0\n",
        "\n",
        "        self._finish_ep()\n",
        "\n",
        "        self.lowlevel = self.ll_env.step(\n",
        "            self.ll_task, seed, config.HRL_STEP_COUNT) if learn_mode else self.ll_env.evaluate(\n",
        "            self.ll_task, config.HRL_STEP_COUNT)\n",
        "\n",
        "        self.goals = self.ll_ctrl.internal_reset(agent, seed, learn_mode)\n",
        "\n",
        "        return self._state(\n",
        "            self.ll_ctrl.get_info(), \n",
        "            None, None, None, None, None,\n",
        "            learn_mode=learn_mode, reset=True, seed=seed)\n",
        "    \n",
        "    def step(self, pi):\n",
        "        a = pi[:, :pi.shape[1]//3].clone()\n",
        "\n",
        "        self.ll_ctrl.set_goals(a.clone())\n",
        "        base_states = self.ll_ctrl.get_info().states.clone()\n",
        "\n",
        "        (log_prob, _, _, _, _, ll_actions, _, good), acu_reward = next(self.lowlevel)\n",
        "        \n",
        "        next_states = self.ll_ctrl.get_info().states.clone()\n",
        "        \n",
        "        goods = select_exp(base_states, next_states)\n",
        "        actions = self.proximaly_close_actions(a, pi, base_states, next_states, goods)\n",
        "\n",
        "        self.einfo = self._state(\n",
        "            self.ll_ctrl.get_info(), \n",
        "            base_states,\n",
        "            acu_reward, actions, goods, \n",
        "            pi,\n",
        "            self.learn_mode, reset=False)\n",
        "\n",
        "        self.einfo.pi[:, actions.shape[-1]:actions.shape[-1]+ll_actions.shape[-1]*2] = torch.cat([ # TODO[ : KICK OFF\n",
        "            log_prob,#torch.ones_like(log_prob ),\n",
        "            ll_actions], 1)#torch.ones_like(actionsZ) ], 1)\n",
        "\n",
        "        return self.einfo\n",
        "    \n",
        "    def proximaly_close_actions(self, a, actions, base_states, next_states, goods):\n",
        "        self.total += len(a)\n",
        "        \n",
        "        if not self.learn_mode:\n",
        "            return a\n",
        "        \n",
        "        pi = Normal(actions[:, a.shape[1]: a.shape[1]*2], actions[:, a.shape[1]*2:])\n",
        "        og = self.ll_ctrl.optimal_goals(base_states, next_states)\n",
        "\n",
        "        baseline = pi.log_prob(a).mean(1) * (1. + 1. - self.ls.get_ls())\n",
        "\n",
        "        idx = pi.log_prob(og).mean(1) > baseline\n",
        "        if not sum(idx):\n",
        "            return a\n",
        "        \n",
        "        a[idx > 0] = og[idx > 0]\n",
        "        self.probed += sum(idx)\n",
        "\n",
        "        if sum(idx) * 2 > len(og) and sum(goods):\n",
        "            return a\n",
        "        \n",
        "        self.ls()    \n",
        "        return a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS6swup7FP8X",
        "scrolled": false,
        "outputId": "c92ba020-1def-4156-e1e8-f76cea9d6731"
      },
      "source": [
        "timebudget.set_quiet()\n",
        "import time\n",
        "env_start = time.time()\n",
        "env_counter = 0\n",
        "\n",
        "import hashlib\n",
        "algo = hashlib.md5()\n",
        "algo.update(open(\"config.py\", \"rb\").read())\n",
        "import logging\n",
        "\n",
        "LOG_NAME = \"stat_\"+config.PREFIX+\"_\"+algo.hexdigest()+\"%i\"%time.time()\n",
        "def log_info():\n",
        "    return LOG_NAME\n",
        "\n",
        "logging.basicConfig(filename=LOG_NAME, level=logging.DEBUG)\n",
        "logging.info(open(\"config.py\", \"r\").read())\n",
        "def callback(bot, task, test_scores, learn_scores, seeds, total):\n",
        "    global env_start, env_counter\n",
        "    env_counter += 1\n",
        "    if test_scores is None:\n",
        "        return\n",
        "    msg = (\"\\n\\t [\", env_counter, \"] < %.2f\"%((time.time()-env_start) / 60), \"min > TEST ==> \", test_scores, \"exploring score:\", learn_scores.mean())\n",
        "    logging.info(msg)\n",
        "    print(*msg)\n",
        "    print(\"debug info ->\", LOG_NAME)\n",
        "    if env_counter > config.TOTAL_ROUNDS:\n",
        "        exit()\n",
        "        import os\n",
        "        os.system(\"sh looper.sh\")\n",
        "\n",
        "from head import install_highlevel\n",
        "#from dlhppo import HighLevelCtrlTask\n",
        "\n",
        "print(\"\\nSTART$log is in : \", log_info())\n",
        "\n",
        "KEYID = config.PREFIX+\"_hl\"\n",
        "high_level_task = HighLevelCtrlTask(\"mujoco_dock\", KEYID)\n",
        "\n",
        "env, task = install_highlevel(high_level_task, KEYID)\n",
        "scores = env.start(task, callback, True)\n",
        "\n",
        "print(\"\\nDONE$log is in : \", log_info())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "START$log is in :  stat_v4_PUSHER_PANDA_ffe589da0d561a123d1aedccc21573d41625088616\n",
            "v4_PUSHER_PANDA_hl_gym_dock_0 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_1 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_2 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_3 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_4 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_5 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_6 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_7 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_8 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_9 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_10 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_11 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_12 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_13 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_14 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_15 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_16 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_17 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_18 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_19 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_20 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_21 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_22 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_23 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_24 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_25 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_26 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_27 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_28 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_29 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_30 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_31 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_32 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_33 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_34 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_35 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_36 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_37 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_38 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_39 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_40 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_41 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_42 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_43 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_44 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_45 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_46 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_47 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_48 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_49 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_50 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_51 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_52 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_53 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_54 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_55 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_56 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_57 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_58 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_59 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_60 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_61 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_62 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_63 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_64 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_65 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_66 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_67 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_68 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_69 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_70 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_71 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_72 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_73 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_74 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_75 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_76 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_77 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_78 create PUSHER_PANDA\n",
            "v4_PUSHER_PANDA_hl_gym_dock_79 create PUSHER_PANDA\n",
            "\n",
            "LOW LEVEL POLICY: \n",
            " [[3280, '<- memory_size;', 1024, '<- batch_size;', 10, '<- optim_epochs;', 2048, '<- optim_batch_size;', 64000, '<- optim_pool_size;', 7, '<- recalc_delay;', 3, '<- sync_delta_a;', 2, '<- sync_delta_c;', 10, '<- learning_delay;', 10, '<- learning_repeat;', 0.0003, '<- lr_actor;', 0.05, '<- tau_actor;', 0.05, '<- tau_critic;', 0.2, '<- ppo_eps;', False, '<- natural;', False, '<- mean_only;']]\n",
            "RELU True [NoisyLinear(in_features=19, out_features=66, bias=False), NoisyLinear(in_features=66, out_features=66, bias=False), NoisyLinear(in_features=66, out_features=66, bias=False), NoisyLinear(in_features=66, out_features=3, bias=False)]\n",
            "Sequential(\n",
            "  (layer_0): Linear(in_features=40, out_features=256, bias=False)\n",
            "  (layer_1): ReLU()\n",
            "  (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (layer_3): Tanh()\n",
            "  (layer_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (layer_5): Tanh()\n",
            "  (layer_6): Linear(in_features=256, out_features=1, bias=False)\n",
            ")\n",
            "Sequential(\n",
            "  (layer_0): Linear(in_features=40, out_features=256, bias=False)\n",
            "  (layer_1): ReLU()\n",
            "  (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (layer_3): Tanh()\n",
            "  (layer_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (layer_5): Tanh()\n",
            "  (layer_6): Linear(in_features=256, out_features=1, bias=False)\n",
            ")\n",
            "RELU True [NoisyLinear(in_features=19, out_features=66, bias=False), NoisyLinear(in_features=66, out_features=66, bias=False), NoisyLinear(in_features=66, out_features=66, bias=False), NoisyLinear(in_features=66, out_features=3, bias=False)]\n",
            "Sequential(\n",
            "  (layer_0): Linear(in_features=40, out_features=256, bias=False)\n",
            "  (layer_1): ReLU()\n",
            "  (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (layer_3): Tanh()\n",
            "  (layer_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (layer_5): Tanh()\n",
            "  (layer_6): Linear(in_features=256, out_features=1, bias=False)\n",
            ")\n",
            "ActorCritic(\n",
            "  (goal_encoder): GoalIdentity()\n",
            "  (encoder): GlobalNormalizerWithTimeEx(\n",
            "    (enc): GlobalNormalizer(\n",
            "      (norm): Normalizer()\n",
            "    )\n",
            "    (goal_encoder): GoalGlobalNorm(\n",
            "      (norm): Normalizer()\n",
            "    )\n",
            "  )\n",
            "  (actor_0): Actor(\n",
            "    (net): NoisyNet(\n",
            "      (sigma): ParameterList(\n",
            "          (0): Parameter containing: [torch.FloatTensor of size 66x19]\n",
            "          (1): Parameter containing: [torch.FloatTensor of size 66x66]\n",
            "          (2): Parameter containing: [torch.FloatTensor of size 66x66]\n",
            "          (3): Parameter containing: [torch.FloatTensor of size 3x66]\n",
            "      )\n",
            "    )\n",
            "    (algo): PPOHead()\n",
            "    (ibottleneck): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=16, bias=False)\n",
            "      (1): Tanh()\n",
            "      (2): Linear(in_features=16, out_features=10, bias=True)\n",
            "      (3): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (critic_0): Critic(\n",
            "    (ibottleneck): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=16, bias=False)\n",
            "      (1): Tanh()\n",
            "      (2): Linear(in_features=16, out_features=10, bias=True)\n",
            "      (3): Tanh()\n",
            "    )\n",
            "    (net): Sequential(\n",
            "      (layer_0): Linear(in_features=40, out_features=256, bias=False)\n",
            "      (layer_1): ReLU()\n",
            "      (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (layer_3): Tanh()\n",
            "      (layer_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (layer_5): Tanh()\n",
            "      (layer_6): Linear(in_features=256, out_features=1, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "ActorCritic(\n",
            "  (goal_encoder): GoalIdentity()\n",
            "  (encoder): GlobalNormalizerWithTimeEx(\n",
            "    (enc): GlobalNormalizer(\n",
            "      (norm): Normalizer()\n",
            "    )\n",
            "    (goal_encoder): GoalGlobalNorm(\n",
            "      (norm): Normalizer()\n",
            "    )\n",
            "  )\n",
            "  (actor_0): Actor(\n",
            "    (net): NoisyNet(\n",
            "      (sigma): ParameterList(\n",
            "          (0): Parameter containing: [torch.FloatTensor of size 66x19]\n",
            "          (1): Parameter containing: [torch.FloatTensor of size 66x66]\n",
            "          (2): Parameter containing: [torch.FloatTensor of size 66x66]\n",
            "          (3): Parameter containing: [torch.FloatTensor of size 3x66]\n",
            "      )\n",
            "    )\n",
            "    (algo): PPOHead()\n",
            "    (ibottleneck): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=16, bias=False)\n",
            "      (1): Tanh()\n",
            "      (2): Linear(in_features=16, out_features=10, bias=True)\n",
            "      (3): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (critic_0): Critic(\n",
            "    (ibottleneck): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=16, bias=False)\n",
            "      (1): Tanh()\n",
            "      (2): Linear(in_features=16, out_features=10, bias=True)\n",
            "      (3): Tanh()\n",
            "    )\n",
            "    (net): Sequential(\n",
            "      (layer_0): Linear(in_features=40, out_features=256, bias=False)\n",
            "      (layer_1): ReLU()\n",
            "      (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (layer_3): Tanh()\n",
            "      (layer_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (layer_5): Tanh()\n",
            "      (layer_6): Linear(in_features=256, out_features=1, bias=False)\n",
            "    )\n",
            "  )\n",
            "  (critic_1): Critic(\n",
            "    (ibottleneck): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=16, bias=False)\n",
            "      (1): Tanh()\n",
            "      (2): Linear(in_features=16, out_features=10, bias=True)\n",
            "      (3): Tanh()\n",
            "    )\n",
            "    (net): Sequential(\n",
            "      (layer_0): Linear(in_features=40, out_features=256, bias=False)\n",
            "      (layer_1): ReLU()\n",
            "      (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (layer_3): Tanh()\n",
            "      (layer_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (layer_5): Tanh()\n",
            "      (layer_6): Linear(in_features=256, out_features=1, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "---> 2 1 2 1 1 1\n",
            "BRAIN OPTIM :  v4_PUSHER_PANDA_ll_checkpoints\n",
            "\n",
            "HIGH LEVEL: \n",
            " [[3200, '<- memory_size;', 8192, '<- batch_size;', 1, '<- optim_epochs;', 16384, '<- optim_batch_size;', 64000, '<- optim_pool_size;', 3, '<- recalc_delay;', 3, '<- sync_delta_a;', 2, '<- sync_delta_c;', 40, '<- learning_delay;', 80, '<- learning_repeat;', 0.0001, '<- lr_actor;', 0.07, '<- tau_actor;', 0.05, '<- tau_critic;', 0.2, '<- ppo_eps;', False, '<- natural;', False, '<- mean_only;']]\n",
            "RELU True [NoisyLinear(in_features=30, out_features=256, bias=False), NoisyLinear(in_features=256, out_features=256, bias=False), NoisyLinear(in_features=256, out_features=256, bias=False), NoisyLinear(in_features=256, out_features=64, bias=False)]\n",
            "RELU True [NoisyLinear(in_features=30, out_features=256, bias=False), NoisyLinear(in_features=256, out_features=256, bias=False), NoisyLinear(in_features=256, out_features=256, bias=False), NoisyLinear(in_features=256, out_features=64, bias=False)]\n",
            "ActorCritic(\n",
            "  (goal_encoder): GoalGlobalNorm(\n",
            "    (norm): Normalizer()\n",
            "  )\n",
            "  (encoder): GlobalNormalizerWithTimeEx(\n",
            "    (enc): GlobalNormalizer(\n",
            "      (norm): Normalizer()\n",
            "    )\n",
            "    (goal_encoder): GoalGlobalNorm(\n",
            "      (norm): Normalizer()\n",
            "    )\n",
            "  )\n",
            "  (actor_0): Actor(\n",
            "    (net): NoisyNet(\n",
            "      (sigma): ParameterList(\n",
            "          (0): Parameter containing: [torch.FloatTensor of size 256x30]\n",
            "          (1): Parameter containing: [torch.FloatTensor of size 256x256]\n",
            "          (2): Parameter containing: [torch.FloatTensor of size 256x256]\n",
            "          (3): Parameter containing: [torch.FloatTensor of size 64x256]\n",
            "      )\n",
            "    )\n",
            "    (algo): PPOHead()\n",
            "  )\n",
            "  (critic_0): Critic(\n",
            "    (ibottleneck): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=16, bias=False)\n",
            "      (1): Tanh()\n",
            "      (2): Linear(in_features=16, out_features=10, bias=True)\n",
            "      (3): Tanh()\n",
            "    )\n",
            "    (net): Sequential(\n",
            "      (layer_0): Linear(in_features=40, out_features=256, bias=False)\n",
            "      (layer_1): ReLU()\n",
            "      (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (layer_3): Tanh()\n",
            "      (layer_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (layer_5): Tanh()\n",
            "      (layer_6): Linear(in_features=256, out_features=1, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "ActorCritic(\n",
            "  (goal_encoder): GoalGlobalNorm(\n",
            "    (norm): Normalizer()\n",
            "  )\n",
            "  (encoder): GlobalNormalizerWithTimeEx(\n",
            "    (enc): GlobalNormalizer(\n",
            "      (norm): Normalizer()\n",
            "    )\n",
            "    (goal_encoder): GoalGlobalNorm(\n",
            "      (norm): Normalizer()\n",
            "    )\n",
            "  )\n",
            "  (actor_0): Actor(\n",
            "    (net): NoisyNet(\n",
            "      (sigma): ParameterList(\n",
            "          (0): Parameter containing: [torch.FloatTensor of size 256x30]\n",
            "          (1): Parameter containing: [torch.FloatTensor of size 256x256]\n",
            "          (2): Parameter containing: [torch.FloatTensor of size 256x256]\n",
            "          (3): Parameter containing: [torch.FloatTensor of size 64x256]\n",
            "      )\n",
            "    )\n",
            "    (algo): PPOHead()\n",
            "  )\n",
            "  (critic_0): Critic(\n",
            "    (ibottleneck): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=16, bias=False)\n",
            "      (1): Tanh()\n",
            "      (2): Linear(in_features=16, out_features=10, bias=True)\n",
            "      (3): Tanh()\n",
            "    )\n",
            "    (net): Sequential(\n",
            "      (layer_0): Linear(in_features=40, out_features=256, bias=False)\n",
            "      (layer_1): ReLU()\n",
            "      (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (layer_3): Tanh()\n",
            "      (layer_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (layer_5): Tanh()\n",
            "      (layer_6): Linear(in_features=256, out_features=1, bias=False)\n",
            "    )\n",
            "  )\n",
            "  (critic_1): Critic(\n",
            "    (ibottleneck): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=16, bias=False)\n",
            "      (1): Tanh()\n",
            "      (2): Linear(in_features=16, out_features=10, bias=True)\n",
            "      (3): Tanh()\n",
            "    )\n",
            "    (net): Sequential(\n",
            "      (layer_0): Linear(in_features=40, out_features=256, bias=False)\n",
            "      (layer_1): ReLU()\n",
            "      (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (layer_3): Tanh()\n",
            "      (layer_4): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (layer_5): Tanh()\n",
            "      (layer_6): Linear(in_features=256, out_features=1, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "---> 2 1 2 1 1 1\n",
            "BRAIN OPTIM :  v4_PUSHER_PANDA_hl_checkpoints\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py:48: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
            "  super(Adam, self).__init__(params, defaults)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "timebudget report...\n",
            "\n",
            "stats :  0 0 0 -0.9 False\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 0 -0.9 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 0 -0.9 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 0 -0.9 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 0 -0.9 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 0 -0.9 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 0 -0.9 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 0 -0.9 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 0 -0.9 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 0 -0.9 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "==============================\n",
            "SUCCES RATIO 0.0% [mean=-40.0]\n",
            "==============================\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 0 -0.9 True\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "[    1>    40:: 0] steps =   40, max_step =   0/ 40, reward=-38.962500 <action=tensor([-0.1248, -0.0338, -0.2425])...>                    \n",
            " PUSH  80 80\n",
            "timebudget report...\n",
            "                    _push:  238.39ms for     80 calls\n",
            "            credit-assign:   11.51ms for   1600 calls\n",
            "                 __call__:   10.64ms for   1600 calls\n",
            "                  shuffle:  474.84ms for      1 calls\n",
            "             recalc_feats:    0.03ms for     80 calls\n",
            "\n",
            "stats :  tensor(3023) 3200 0 -0.9 True\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:40: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " PUSH  80 80\n",
            "DO FINISH\n",
            "[    2>    81:: 0] steps =   40, max_step =  41/ 40, reward=-39.000000 <action=tensor([-0.4788, -0.1485, -0.2653])...>                    \n",
            " PUSH  80 80\n",
            "timebudget report...\n",
            "              learn-round: 1495.60ms for     84 calls\n",
            "                    learn: 1495.59ms for     84 calls\n",
            "                   _learn: 1046.79ms for    120 calls\n",
            "          _learn_backprop:  788.15ms for    120 calls\n",
            "                 backprop:  152.91ms for    360 calls\n",
            "                    _push:  188.75ms for    160 calls\n",
            "            credit-assign:    9.14ms for   3200 calls\n",
            "            _learn_future:  242.06ms for    120 calls\n",
            "                 __call__:    6.33ms for   3200 calls\n",
            "              update_goal:    4.03ms for   1600 calls\n",
            "                  shuffle:  353.27ms for      2 calls\n",
            "              _learn_meta:    1.45ms for    120 calls\n",
            "        FastMemory-sample:    0.09ms for    120 calls\n",
            "FastMemory-sample::INDEXING:    0.07ms for    120 calls\n",
            "             recalc_feats:    0.03ms for    160 calls\n",
            "           _learn_debatch:    0.00ms for    120 calls\n",
            "\n",
            "stats :  tensor(2906) 3200 0 -0.9 True\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "\n",
            " PUSH  80 80\n",
            "DO FINISH\n",
            "[    3>   122:: 0] steps =   40, max_step =  41/ 40, reward=-38.950000 <action=tensor([ 0.1648,  0.1304, -0.4404])...>                    \n",
            " PUSH  80 80\n",
            "timebudget report...\n",
            "              learn-round: 1492.76ms for     84 calls\n",
            "                    learn: 1492.75ms for     84 calls\n",
            "                   _learn: 1044.80ms for    120 calls\n",
            "          _learn_backprop:  786.09ms for    120 calls\n",
            "                 backprop:  152.67ms for    360 calls\n",
            "                    _push:  192.42ms for    160 calls\n",
            "            credit-assign:    9.29ms for   3200 calls\n",
            "            _learn_future:  241.36ms for    120 calls\n",
            "                 __call__:    6.43ms for   3200 calls\n",
            "              update_goal:    4.09ms for   1600 calls\n",
            "                  shuffle:  532.14ms for      2 calls\n",
            "              _learn_meta:    1.46ms for    120 calls\n",
            "        FastMemory-sample:    0.09ms for    120 calls\n",
            "FastMemory-sample::INDEXING:    0.07ms for    120 calls\n",
            "             recalc_feats:    0.03ms for    160 calls\n",
            "           _learn_debatch:    0.00ms for    120 calls\n",
            "\n",
            "stats :  tensor(2847) 3200 0 -0.9 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "\n",
            " PUSH  80 80\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "                    _push:  136.92ms for     80 calls\n",
            "            credit-assign:    6.59ms for   1600 calls\n",
            "              update_goal:    4.10ms for   1600 calls\n",
            "                 __call__:    1.71ms for   1600 calls\n",
            "                  shuffle:  258.27ms for      1 calls\n",
            "             recalc_feats:    0.02ms for     80 calls\n",
            "\n",
            "stats :  0 3200 0 -0.9 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 0 -0.9 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 0 -0.9 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 0 -0.9 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 0 -0.9 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 0 -0.9 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 0 -0.9 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 0 -0.9 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 0 -0.9 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "==============================\n",
            "SUCCES RATIO 0.0% [mean=-40.0]\n",
            "==============================\n",
            "\n",
            "\t [ 3 ] < 7.38 min > TEST ==>  [[tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64)]] exploring score: tensor(-40.9500, dtype=torch.float64)\n",
            "debug info -> stat_v4_PUSHER_PANDA_ffe589da0d561a123d1aedccc21573d41625088616\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 0 -0.9 True\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "[    4>   163:: 0] steps =   40, max_step =  41/ 40, reward=-38.925000 <action=tensor([-0.3773, -0.1872, -0.0193])...>                    \n",
            " PUSH  80 80\n",
            "timebudget report...\n",
            "              learn-round: 1535.48ms for     84 calls\n",
            "                    learn: 1535.46ms for     84 calls\n",
            "                   _learn: 1074.70ms for    120 calls\n",
            "          _learn_backprop:  810.22ms for    120 calls\n",
            "                 backprop:  158.13ms for    360 calls\n",
            "            _learn_future:  246.52ms for    120 calls\n",
            "                    _push:  241.80ms for     80 calls\n",
            "            credit-assign:   11.69ms for   1600 calls\n",
            "                 __call__:   10.83ms for   1600 calls\n",
            "                  shuffle:  503.74ms for      1 calls\n",
            "              _learn_meta:    1.45ms for    120 calls\n",
            "        FastMemory-sample:    0.10ms for    120 calls\n",
            "FastMemory-sample::INDEXING:    0.07ms for    120 calls\n",
            "             recalc_feats:    0.03ms for     80 calls\n",
            "           _learn_debatch:    0.00ms for    120 calls\n",
            "\n",
            "stats :  tensor(2833) 3200 0 -0.9 True\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "\n",
            " PUSH  80 80\n",
            "DO FINISH\n",
            "[    5>   204:: 0] steps =   40, max_step =  41/ 40, reward=-38.925000 <action=tensor([ 0.5732,  0.6169, -0.2404])...>                    \n",
            " PUSH  80 80\n",
            "timebudget report...\n",
            "              learn-round: 1512.60ms for     84 calls\n",
            "                    learn: 1512.58ms for     84 calls\n",
            "                   _learn: 1058.68ms for    120 calls\n",
            "          _learn_backprop:  798.37ms for    120 calls\n",
            "                 backprop:  155.88ms for    360 calls\n",
            "                    _push:  189.55ms for    160 calls\n",
            "            credit-assign:    9.16ms for   3200 calls\n",
            "            _learn_future:  242.97ms for    120 calls\n",
            "                 __call__:    6.24ms for   3200 calls\n",
            "              update_goal:    4.19ms for   1600 calls\n",
            "                  shuffle:  365.80ms for      2 calls\n",
            "              _learn_meta:    1.48ms for    120 calls\n",
            "        FastMemory-sample:    0.10ms for    120 calls\n",
            "FastMemory-sample::INDEXING:    0.08ms for    120 calls\n",
            "             recalc_feats:    0.03ms for    160 calls\n",
            "           _learn_debatch:    0.00ms for    120 calls\n",
            "\n",
            "stats :  tensor(2786) 3200 0 -0.9 True\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "\n",
            " PUSH  80 80\n",
            "DO FINISH\n",
            "[    6>   245:: 0] steps =   40, max_step =  41/ 40, reward=-38.950000 <action=tensor([-0.0531,  0.8042, -0.6131])...>                    \n",
            " PUSH  80 80\n",
            "timebudget report...\n",
            "              learn-round: 1517.17ms for     84 calls\n",
            "                    learn: 1517.15ms for     84 calls\n",
            "                   _learn: 1061.88ms for    120 calls\n",
            "          _learn_backprop:  800.82ms for    120 calls\n",
            "                 backprop:  156.25ms for    360 calls\n",
            "                    _push:  192.08ms for    160 calls\n",
            "            credit-assign:    9.28ms for   3200 calls\n",
            "            _learn_future:  243.68ms for    120 calls\n",
            "                 __call__:    6.39ms for   3200 calls\n",
            "              update_goal:    4.10ms for   1600 calls\n",
            "                  shuffle:  383.77ms for      2 calls\n",
            "              _learn_meta:    1.44ms for    120 calls\n",
            "        FastMemory-sample:    0.09ms for    120 calls\n",
            "FastMemory-sample::INDEXING:    0.07ms for    120 calls\n",
            "             recalc_feats:    0.03ms for    160 calls\n",
            "           _learn_debatch:    0.00ms for    120 calls\n",
            "\n",
            "stats :  tensor(2670) 3200 0 -0.9 True\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "\n",
            " PUSH  80 80\n",
            "DO FINISH\n",
            "[    7>   280:: 0] steps =   34, max_step =  41/ 40, reward=-33.000000 <action=tensor([ 0.0177,  1.0203, -1.0829])...>                    PPO too off, from sampled actions, policies problems!!  8192 20 1217 tensor(-2.6003, grad_fn=<MeanBackward0>) tensor(-0.4464)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 40 1217 tensor(-2.5724, grad_fn=<MeanBackward0>) tensor(-0.4451)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 60 1217 tensor(-2.5609, grad_fn=<MeanBackward0>) tensor(-0.4454)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 80 1217 tensor(-2.5641, grad_fn=<MeanBackward0>) tensor(-0.4388)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 100 1217 tensor(-2.5289, grad_fn=<MeanBackward0>) tensor(-0.4466)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 120 1217 tensor(-2.5053, grad_fn=<MeanBackward0>) tensor(-0.4449)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 140 1217 tensor(-2.5019, grad_fn=<MeanBackward0>) tensor(-0.4448)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 160 1217 tensor(-2.5004, grad_fn=<MeanBackward0>) tensor(-0.4452)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 180 1217 tensor(-2.5095, grad_fn=<MeanBackward0>) tensor(-0.4417)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 200 1217 tensor(-2.4748, grad_fn=<MeanBackward0>) tensor(-0.4458)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 220 1217 tensor(-2.5029, grad_fn=<MeanBackward0>) tensor(-0.4385)\n",
            "[    7>   286:: 0] steps =   40, max_step =  41/ 40, reward=-38.950000 <action=tensor([ 0.6940, -0.2784, -0.8393])...>                    \n",
            " PUSH  80 80\n",
            "timebudget report...\n",
            "              learn-round: 1532.21ms for     85 calls\n",
            "                    learn: 1532.19ms for     85 calls\n",
            "                   _learn: 1001.69ms for    130 calls\n",
            "          _learn_backprop:  756.52ms for    130 calls\n",
            "                 backprop:  148.55ms for    390 calls\n",
            "                    _push:  192.68ms for    160 calls\n",
            "            credit-assign:    9.30ms for   3200 calls\n",
            "            _learn_future:  228.44ms for    130 calls\n",
            "                 __call__:    6.26ms for   3200 calls\n",
            "              update_goal:    4.38ms for   1600 calls\n",
            "                  shuffle:  457.05ms for      2 calls\n",
            "              _learn_meta:    1.46ms for    130 calls\n",
            "        FastMemory-sample:    0.10ms for    130 calls\n",
            "FastMemory-sample::INDEXING:    0.07ms for    130 calls\n",
            "             recalc_feats:    0.03ms for    160 calls\n",
            "           _learn_debatch:    0.00ms for    130 calls\n",
            "\n",
            "stats :  tensor(2562) 3200 1 -0.89825 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "\n",
            " PUSH  80 80\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "                    _push:  141.37ms for     80 calls\n",
            "            credit-assign:    6.82ms for   1600 calls\n",
            "              update_goal:    4.25ms for   1600 calls\n",
            "                 __call__:    1.73ms for   1600 calls\n",
            "                  shuffle:  236.61ms for      1 calls\n",
            "             recalc_feats:    0.03ms for     80 calls\n",
            "\n",
            "stats :  0 3200 1 -0.89825 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 1 -0.89825 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 1 -0.89825 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 1 -0.89825 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 1 -0.89825 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 1 -0.89825 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 1 -0.89825 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 1 -0.89825 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 1 -0.89825 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "==============================\n",
            "SUCCES RATIO 0.0% [mean=-40.0]\n",
            "==============================\n",
            "\n",
            "\t [ 7 ] < 18.78 min > TEST ==>  [[tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64)]] exploring score: tensor(-40.9000, dtype=torch.float64)\n",
            "debug info -> stat_v4_PUSHER_PANDA_ffe589da0d561a123d1aedccc21573d41625088616\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 1 -0.89825 True\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "[    8>   327:: 0] steps =   40, max_step =  41/ 40, reward=-38.775000 <action=tensor([ 0.7869, -0.0179, -0.1537])...>                    \n",
            " PUSH  80 80\n",
            "timebudget report...\n",
            "              learn-round: 1536.55ms for     84 calls\n",
            "                    learn: 1536.53ms for     84 calls\n",
            "                   _learn: 1075.44ms for    120 calls\n",
            "          _learn_backprop:  812.73ms for    120 calls\n",
            "                 backprop:  159.56ms for    360 calls\n",
            "            _learn_future:  244.78ms for    120 calls\n",
            "                    _push:  238.06ms for     80 calls\n",
            "            credit-assign:   11.49ms for   1600 calls\n",
            "                 __call__:   10.61ms for   1600 calls\n",
            "                  shuffle:  490.96ms for      1 calls\n",
            "              _learn_meta:    1.51ms for    120 calls\n",
            "        FastMemory-sample:    0.10ms for    120 calls\n",
            "FastMemory-sample::INDEXING:    0.07ms for    120 calls\n",
            "             recalc_feats:    0.03ms for     80 calls\n",
            "           _learn_debatch:    0.00ms for    120 calls\n",
            "\n",
            "stats :  tensor(2722) 3200 1 -0.89825 True\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "\n",
            " PUSH  80 80\n",
            "DO FINISH\n",
            "[    9>   368:: 0] steps =   40, max_step =  41/ 40, reward=-38.900000 <action=tensor([ 0.5401, -0.3034,  0.6084])...>                    \n",
            " PUSH  80 80\n",
            "timebudget report...\n",
            "              learn-round: 1551.79ms for     84 calls\n",
            "                    learn: 1551.77ms for     84 calls\n",
            "                   _learn: 1086.11ms for    120 calls\n",
            "          _learn_backprop:  820.36ms for    120 calls\n",
            "                 backprop:  161.61ms for    360 calls\n",
            "                    _push:  194.47ms for    160 calls\n",
            "            credit-assign:    9.38ms for   3200 calls\n",
            "            _learn_future:  247.06ms for    120 calls\n",
            "                 __call__:    6.39ms for   3200 calls\n",
            "              update_goal:    4.27ms for   1600 calls\n",
            "                  shuffle:  362.63ms for      2 calls\n",
            "              _learn_meta:    1.50ms for    120 calls\n",
            "        FastMemory-sample:    0.10ms for    120 calls\n",
            "FastMemory-sample::INDEXING:    0.08ms for    120 calls\n",
            "             recalc_feats:    0.03ms for    160 calls\n",
            "           _learn_debatch:    0.00ms for    120 calls\n",
            "\n",
            "stats :  tensor(2628) 3200 2 -0.8965 True\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "\n",
            " PUSH  80 80\n",
            "DO FINISH\n",
            "[   10>   409:: 0] steps =   40, max_step =  41/ 40, reward=-39.000000 <action=tensor([ 0.1707,  0.9121, -0.7172])...>                    \n",
            " PUSH  80 80\n",
            "timebudget report...\n",
            "              learn-round: 1520.10ms for     84 calls\n",
            "                    learn: 1520.09ms for     84 calls\n",
            "                   _learn: 1063.94ms for    120 calls\n",
            "          _learn_backprop:  805.38ms for    120 calls\n",
            "                 backprop:  158.14ms for    360 calls\n",
            "                    _push:  192.23ms for    160 calls\n",
            "            credit-assign:    9.28ms for   3200 calls\n",
            "            _learn_future:  241.39ms for    120 calls\n",
            "                 __call__:    6.39ms for   3200 calls\n",
            "              update_goal:    4.12ms for   1600 calls\n",
            "                  shuffle:  355.30ms for      2 calls\n",
            "              _learn_meta:    1.45ms for    120 calls\n",
            "        FastMemory-sample:    0.09ms for    120 calls\n",
            "FastMemory-sample::INDEXING:    0.07ms for    120 calls\n",
            "             recalc_feats:    0.03ms for    160 calls\n",
            "           _learn_debatch:    0.00ms for    120 calls\n",
            "\n",
            "stats :  tensor(2678) 3200 4 -0.893 True\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "\n",
            " PUSH  80 80\n",
            "DO FINISH\n",
            "[   11>   450:: 0] steps =   40, max_step =  41/ 40, reward=-39.000000 <action=tensor([ 0.0536,  1.2632, -1.3702])...>                    \n",
            " PUSH  80 80\n",
            "timebudget report...\n",
            "              learn-round: 1532.39ms for     84 calls\n",
            "                    learn: 1532.38ms for     84 calls\n",
            "                   _learn: 1072.54ms for    120 calls\n",
            "          _learn_backprop:  811.42ms for    120 calls\n",
            "                 backprop:  159.92ms for    360 calls\n",
            "                    _push:  190.28ms for    160 calls\n",
            "            credit-assign:    9.18ms for   3200 calls\n",
            "            _learn_future:  244.08ms for    120 calls\n",
            "                 __call__:    6.27ms for   3200 calls\n",
            "              update_goal:    4.14ms for   1600 calls\n",
            "                  shuffle:  393.11ms for      2 calls\n",
            "              _learn_meta:    1.46ms for    120 calls\n",
            "        FastMemory-sample:    0.09ms for    120 calls\n",
            "FastMemory-sample::INDEXING:    0.07ms for    120 calls\n",
            "             recalc_feats:    0.03ms for    160 calls\n",
            "           _learn_debatch:    0.00ms for    120 calls\n",
            "\n",
            "stats :  tensor(2797) 3200 4 -0.893 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "\n",
            " PUSH  80 80\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "                    _push:  140.85ms for     80 calls\n",
            "            credit-assign:    6.80ms for   1600 calls\n",
            "              update_goal:    4.24ms for   1600 calls\n",
            "                 __call__:    1.73ms for   1600 calls\n",
            "              learn-round: 1244.38ms for      1 calls\n",
            "                    learn: 1244.36ms for      1 calls\n",
            "                   _learn:  124.31ms for     10 calls\n",
            "          _learn_backprop:   93.94ms for     10 calls\n",
            "                 backprop:   17.64ms for     30 calls\n",
            "            _learn_future:   27.03ms for     10 calls\n",
            "                  shuffle:  236.70ms for      1 calls\n",
            "              _learn_meta:    1.12ms for     10 calls\n",
            "             recalc_feats:    0.03ms for     80 calls\n",
            "        FastMemory-sample:    0.09ms for     10 calls\n",
            "FastMemory-sample::INDEXING:    0.07ms for     10 calls\n",
            "           _learn_debatch:    0.00ms for     10 calls\n",
            "\n",
            "stats :  0 3200 4 -0.893 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 4 -0.893 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 4 -0.893 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 4 -0.893 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 4 -0.893 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 4 -0.893 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 4 -0.893 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 4 -0.893 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 4 -0.893 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "==============================\n",
            "SUCCES RATIO 0.0% [mean=-40.0]\n",
            "==============================\n",
            "\n",
            "\t [ 11 ] < 30.26 min > TEST ==>  [[tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64)]] exploring score: tensor(-41., dtype=torch.float64)\n",
            "debug info -> stat_v4_PUSHER_PANDA_ffe589da0d561a123d1aedccc21573d41625088616\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 4 -0.893 True\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "[   12>   491:: 0] steps =   40, max_step =  41/ 40, reward=-39.000000 <action=tensor([-0.1024, -0.2345,  0.8484])...>                    \n",
            " PUSH  80 80\n",
            "timebudget report...\n",
            "              learn-round: 1571.89ms for     84 calls\n",
            "                    learn: 1571.87ms for     84 calls\n",
            "                   _learn: 1100.17ms for    120 calls\n",
            "          _learn_backprop:  825.19ms for    120 calls\n",
            "                 backprop:  162.41ms for    360 calls\n",
            "            _learn_future:  256.79ms for    120 calls\n",
            "                    _push:  238.62ms for     80 calls\n",
            "            credit-assign:   11.51ms for   1600 calls\n",
            "                 __call__:   10.62ms for   1600 calls\n",
            "                  shuffle:  492.24ms for      1 calls\n",
            "              _learn_meta:    1.49ms for    120 calls\n",
            "        FastMemory-sample:    0.10ms for    120 calls\n",
            "FastMemory-sample::INDEXING:    0.08ms for    120 calls\n",
            "             recalc_feats:    0.03ms for     80 calls\n",
            "           _learn_debatch:    0.00ms for    120 calls\n",
            "\n",
            "stats :  tensor(2664) 3200 4 -0.893 True\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "\n",
            " PUSH  80 80\n",
            "DO FINISH\n",
            "[   13>   532:: 0] steps =   40, max_step =  41/ 40, reward=-39.000000 <action=tensor([-0.1078,  1.2689, -1.0256])...>                    \n",
            " PUSH  80 80\n",
            "timebudget report...\n",
            "              learn-round: 1527.42ms for     84 calls\n",
            "                    learn: 1527.40ms for     84 calls\n",
            "                   _learn: 1069.05ms for    120 calls\n",
            "          _learn_backprop:  808.79ms for    120 calls\n",
            "                 backprop:  159.35ms for    360 calls\n",
            "                    _push:  192.47ms for    160 calls\n",
            "            credit-assign:    9.29ms for   3200 calls\n",
            "            _learn_future:  241.97ms for    120 calls\n",
            "                 __call__:    6.35ms for   3200 calls\n",
            "              update_goal:    4.19ms for   1600 calls\n",
            "                  shuffle:  360.44ms for      2 calls\n",
            "              _learn_meta:    1.45ms for    120 calls\n",
            "        FastMemory-sample:    0.10ms for    120 calls\n",
            "FastMemory-sample::INDEXING:    0.07ms for    120 calls\n",
            "             recalc_feats:    0.03ms for    160 calls\n",
            "           _learn_debatch:    0.00ms for    120 calls\n",
            "\n",
            "stats :  tensor(2784) 3200 5 -0.89125 True\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "\n",
            " PUSH  80 80\n",
            "DO FINISH\n",
            "[   14>   560:: 0] steps =   27, max_step =  41/ 40, reward=-26.000000 <action=tensor([-1.0291, -0.3042,  1.2903])...>                    PPO too off, from sampled actions, policies problems!!  8192 240 2683 tensor(-2.5204, grad_fn=<MeanBackward0>) tensor(-0.4460)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 260 2683 tensor(-2.8336, grad_fn=<MeanBackward0>) tensor(-0.4514)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 280 2683 tensor(-2.5905, grad_fn=<MeanBackward0>) tensor(-0.4456)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 300 2683 tensor(-2.6331, grad_fn=<MeanBackward0>) tensor(-0.4485)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 320 2683 tensor(-2.5815, grad_fn=<MeanBackward0>) tensor(-0.4458)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 340 2683 tensor(-2.5788, grad_fn=<MeanBackward0>) tensor(-0.4468)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 360 2683 tensor(-2.5615, grad_fn=<MeanBackward0>) tensor(-0.4456)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 380 2683 tensor(-2.5558, grad_fn=<MeanBackward0>) tensor(-0.4456)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 400 2683 tensor(-2.5412, grad_fn=<MeanBackward0>) tensor(-0.4456)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 420 2683 tensor(-2.5460, grad_fn=<MeanBackward0>) tensor(-0.4461)\n",
            "[   14>   573:: 0] steps =   40, max_step =  41/ 40, reward=-39.000000 <action=tensor([-0.4232, -0.1544,  0.4616])...>                    \n",
            " PUSH  80 80\n",
            "timebudget report...\n",
            "              learn-round: 1531.14ms for     84 calls\n",
            "                    learn: 1531.12ms for     84 calls\n",
            "                   _learn: 1071.66ms for    120 calls\n",
            "          _learn_backprop:  812.04ms for    120 calls\n",
            "                 backprop:  159.82ms for    360 calls\n",
            "                    _push:  198.08ms for    160 calls\n",
            "            credit-assign:    9.57ms for   3200 calls\n",
            "            _learn_future:  241.87ms for    120 calls\n",
            "                 __call__:    6.64ms for   3200 calls\n",
            "              update_goal:    4.20ms for   1600 calls\n",
            "                  shuffle:  375.45ms for      2 calls\n",
            "              _learn_meta:    1.42ms for    120 calls\n",
            "        FastMemory-sample:    0.09ms for    120 calls\n",
            "FastMemory-sample::INDEXING:    0.07ms for    120 calls\n",
            "             recalc_feats:    0.03ms for    160 calls\n",
            "           _learn_debatch:    0.00ms for    120 calls\n",
            "\n",
            "stats :  tensor(2108) 3200 16 -0.872 True\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "\n",
            " PUSH  80 80\n",
            "DO FINISH\n",
            "[   15>   600:: 0] steps =   26, max_step =  41/ 40, reward=-24.950000 <action=tensor([ 0.9378, -0.8630, -1.1306])...>                    PPO too off, from sampled actions, policies problems!!  8192 440 2703 tensor(-2.6575, grad_fn=<MeanBackward0>) tensor(-0.5227)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 460 2706 tensor(-2.5172, grad_fn=<MeanBackward0>) tensor(-0.4749)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 480 2712 tensor(-2.8661, grad_fn=<MeanBackward0>) tensor(-0.5302)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 500 2715 tensor(-2.8609, grad_fn=<MeanBackward0>) tensor(-0.5263)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 520 2721 tensor(-2.8648, grad_fn=<MeanBackward0>) tensor(-0.5289)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 540 2727 tensor(-2.8791, grad_fn=<MeanBackward0>) tensor(-0.5300)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 560 2730 tensor(-2.7898, grad_fn=<MeanBackward0>) tensor(-0.5157)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 580 2733 tensor(-2.8043, grad_fn=<MeanBackward0>) tensor(-0.5200)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 600 2736 tensor(-2.7622, grad_fn=<MeanBackward0>) tensor(-0.5123)\n",
            "[   15>   614:: 0] steps =   40, max_step =  41/ 40, reward=-38.950000 <action=tensor([ 0.8942,  1.3822, -1.4281])...>                    \n",
            " PUSH  80 80\n",
            "timebudget report...\n",
            "              learn-round: 1532.54ms for     84 calls\n",
            "                    learn: 1532.52ms for     84 calls\n",
            "                   _learn: 1072.63ms for    120 calls\n",
            "          _learn_backprop:  811.23ms for    120 calls\n",
            "                 backprop:  159.47ms for    360 calls\n",
            "                    _push:  187.85ms for    160 calls\n",
            "            _learn_future:  243.23ms for    120 calls\n",
            "            credit-assign:    9.07ms for   3200 calls\n",
            "                 __call__:    6.19ms for   3200 calls\n",
            "              update_goal:    4.13ms for   1600 calls\n",
            "                  shuffle:  367.61ms for      2 calls\n",
            "              _learn_meta:    1.47ms for    120 calls\n",
            "        FastMemory-sample:    0.10ms for    120 calls\n",
            "FastMemory-sample::INDEXING:    0.07ms for    120 calls\n",
            "             recalc_feats:    0.03ms for    160 calls\n",
            "           _learn_debatch:    0.00ms for    120 calls\n",
            "\n",
            "stats :  tensor(1892) 3200 27 -0.85275 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "\n",
            " PUSH  80 80\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "                    _push:  138.92ms for     80 calls\n",
            "            credit-assign:    6.70ms for   1600 calls\n",
            "              update_goal:    4.11ms for   1600 calls\n",
            "                 __call__:    1.82ms for   1600 calls\n",
            "                  shuffle:  229.52ms for      1 calls\n",
            "             recalc_feats:    0.02ms for     80 calls\n",
            "\n",
            "stats :  0 3200 27 -0.85275 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 27 -0.85275 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 27 -0.85275 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 27 -0.85275 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 27 -0.85275 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 27 -0.85275 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 27 -0.85275 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 27 -0.85275 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 27 -0.85275 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "==============================\n",
            "SUCCES RATIO 0.0% [mean=-40.0]\n",
            "==============================\n",
            "\n",
            "\t [ 15 ] < 41.78 min > TEST ==>  [[tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64), tensor(-40., dtype=torch.float64)]] exploring score: tensor(-40.9500, dtype=torch.float64)\n",
            "debug info -> stat_v4_PUSHER_PANDA_ffe589da0d561a123d1aedccc21573d41625088616\n",
            "timebudget report...\n",
            "\n",
            "stats :  0 3200 27 -0.85275 True\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "[   16>   640:: 0] steps =   25, max_step =  41/ 40, reward=-24.000000 <action=tensor([-1.0841, -1.2975,  1.4672])...>                    PPO too off, from sampled actions, policies problems!!  8192 620 2766 tensor(-2.7141, grad_fn=<MeanBackward0>) tensor(-0.5904)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 640 2775 tensor(-2.7714, grad_fn=<MeanBackward0>) tensor(-0.5896)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 660 2778 tensor(-2.7952, grad_fn=<MeanBackward0>) tensor(-0.5906)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 680 2790 tensor(-2.6376, grad_fn=<MeanBackward0>) tensor(-0.5757)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 700 2793 tensor(-2.7928, grad_fn=<MeanBackward0>) tensor(-0.5930)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 720 2805 tensor(-2.7860, grad_fn=<MeanBackward0>) tensor(-0.5891)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 740 2808 tensor(-2.7749, grad_fn=<MeanBackward0>) tensor(-0.5928)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 760 2811 tensor(-2.7702, grad_fn=<MeanBackward0>) tensor(-0.5909)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 780 2817 tensor(-2.7765, grad_fn=<MeanBackward0>) tensor(-0.5884)\n",
            "[   16>   655:: 0] steps =   40, max_step =  41/ 40, reward=-38.987500 <action=tensor([-0.8622, -0.4967,  1.0312])...>                    \n",
            " PUSH  80 80\n",
            "timebudget report...\n",
            "              learn-round: 1522.04ms for     84 calls\n",
            "                    learn: 1522.03ms for     84 calls\n",
            "                   _learn: 1065.30ms for    120 calls\n",
            "          _learn_backprop:  799.27ms for    120 calls\n",
            "                 backprop:  157.57ms for    360 calls\n",
            "            _learn_future:  249.12ms for    120 calls\n",
            "                    _push:  236.57ms for     80 calls\n",
            "            credit-assign:   11.43ms for   1600 calls\n",
            "                 __call__:   10.64ms for   1600 calls\n",
            "                  shuffle:  469.04ms for      1 calls\n",
            "              _learn_meta:    1.43ms for    120 calls\n",
            "        FastMemory-sample:    0.09ms for    120 calls\n",
            "FastMemory-sample::INDEXING:    0.07ms for    120 calls\n",
            "             recalc_feats:    0.03ms for     80 calls\n",
            "           _learn_debatch:    0.00ms for    120 calls\n",
            "\n",
            "stats :  tensor(2570) 3200 28 -0.851 True\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "\n",
            " PUSH  80 80\n",
            "DO FINISH\n",
            "[   17>   680:: 0] steps =   24, max_step =  41/ 40, reward=-23.000000 <action=tensor([ 1.1389,  1.4031, -1.5441])...>                    PPO too off, from sampled actions, policies problems!!  8192 800 2824 tensor(-4.2480, grad_fn=<MeanBackward0>) tensor(-0.5787)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 820 2824 tensor(-4.2550, grad_fn=<MeanBackward0>) tensor(-0.4806)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 840 2824 tensor(-4.2582, grad_fn=<MeanBackward0>) tensor(-0.4782)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 860 2824 tensor(-4.2527, grad_fn=<MeanBackward0>) tensor(-0.4803)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 880 2824 tensor(-4.2685, grad_fn=<MeanBackward0>) tensor(-0.4807)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 900 2824 tensor(-4.2483, grad_fn=<MeanBackward0>) tensor(-0.4798)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 920 2824 tensor(-4.3525, grad_fn=<MeanBackward0>) tensor(-0.5575)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 940 2824 tensor(-4.2325, grad_fn=<MeanBackward0>) tensor(-0.4825)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 960 2824 tensor(-4.2291, grad_fn=<MeanBackward0>) tensor(-0.4804)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 980 2824 tensor(-4.2534, grad_fn=<MeanBackward0>) tensor(-0.4818)\n",
            "PPO too off, from sampled actions, policies problems!!  8192 1000 2824 tensor(-4.2389, grad_fn=<MeanBackward0>) tensor(-0.4834)\n",
            "[   17>   696:: 0] steps =   40, max_step =  41/ 40, reward=-39.000000 <action=tensor([ 0.9892,  0.7868, -0.7705])...>                    \n",
            " PUSH  80 80\n",
            "timebudget report...\n",
            "              learn-round: 1527.59ms for     85 calls\n",
            "                    learn: 1527.58ms for     85 calls\n",
            "                   _learn:  998.68ms for    130 calls\n",
            "          _learn_backprop:  749.62ms for    130 calls\n",
            "                 backprop:  147.59ms for    390 calls\n",
            "            _learn_future:  233.09ms for    130 calls\n",
            "                    _push:  183.90ms for    160 calls\n",
            "            credit-assign:    8.88ms for   3200 calls\n",
            "                 __call__:    6.17ms for   3200 calls\n",
            "              update_goal:    3.86ms for   1600 calls\n",
            "                  shuffle:  354.88ms for      2 calls\n",
            "              _learn_meta:    1.39ms for    130 calls\n",
            "        FastMemory-sample:    0.09ms for    130 calls\n",
            "FastMemory-sample::INDEXING:    0.07ms for    130 calls\n",
            "             recalc_feats:    0.03ms for    160 calls\n",
            "           _learn_debatch:    0.00ms for    130 calls\n",
            "\n",
            "stats :  tensor(1852) 3200 40 -0.83 True\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "\n",
            " PUSH  80 80\n",
            "DO FINISH\n",
            "[   18>   720:: 0] steps =   23, max_step =  41/ 40, reward=-22.000000 <action=tensor([1.0612, 1.4542, 0.2355])...>                    PPO too off, from sampled actions, policies problems!!  8192 1020 3052 tensor(-2.7077, grad_fn=<MeanBackward0>) tensor(-0.6194)\n",
            "[   18>   737:: 0] steps =   40, max_step =  41/ 40, reward=-39.000000 <action=tensor([-0.2378,  0.5461,  0.9909])...>                    \n",
            " PUSH  80 80\n",
            "timebudget report...\n",
            "              learn-round: 1526.21ms for     84 calls\n",
            "                    learn: 1526.20ms for     84 calls\n",
            "                   _learn: 1068.21ms for    120 calls\n",
            "          _learn_backprop:  806.22ms for    120 calls\n",
            "                 backprop:  158.59ms for    360 calls\n",
            "                    _push:  190.38ms for    160 calls\n",
            "            credit-assign:    9.19ms for   3200 calls\n",
            "            _learn_future:  244.11ms for    120 calls\n",
            "                 __call__:    6.43ms for   3200 calls\n",
            "              update_goal:    3.91ms for   1600 calls\n",
            "                  shuffle:  355.72ms for      2 calls\n",
            "              _learn_meta:    1.43ms for    120 calls\n",
            "        FastMemory-sample:    0.09ms for    120 calls\n",
            "FastMemory-sample::INDEXING:    0.07ms for    120 calls\n",
            "             recalc_feats:    0.03ms for    160 calls\n",
            "           _learn_debatch:    0.00ms for    120 calls\n",
            "\n",
            "stats :  tensor(2460) 3200 44 -0.823 True\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n",
            "\n",
            " PUSH  80 80\n",
            "DO FINISH\n",
            "[   19>   778:: 0] steps =   40, max_step =  41/ 40, reward=-39.000000 <action=tensor([ 0.2470,  0.9925, -1.3765])...>                    \n",
            " PUSH  80 80\n",
            "timebudget report...\n",
            "              learn-round: 1513.59ms for     84 calls\n",
            "                    learn: 1513.58ms for     84 calls\n",
            "                   _learn: 1059.38ms for    120 calls\n",
            "          _learn_backprop:  802.43ms for    120 calls\n",
            "                 backprop:  157.95ms for    360 calls\n",
            "                    _push:  188.04ms for    160 calls\n",
            "            credit-assign:    9.08ms for   3200 calls\n",
            "            _learn_future:  239.61ms for    120 calls\n",
            "                 __call__:    6.27ms for   3200 calls\n",
            "              update_goal:    4.01ms for   1600 calls\n",
            "                  shuffle:  392.19ms for      2 calls\n",
            "              _learn_meta:    1.45ms for    120 calls\n",
            "        FastMemory-sample:    0.09ms for    120 calls\n",
            "FastMemory-sample::INDEXING:    0.07ms for    120 calls\n",
            "             recalc_feats:    0.03ms for    160 calls\n",
            "           _learn_debatch:    0.00ms for    120 calls\n",
            "\n",
            "stats :  tensor(2753) 3200 45 -0.82125 False\n",
            "\n",
            " ep selection ---> 80\n",
            "DO FINISH\n",
            "DO FINISH\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my1KUtx3FP8d"
      },
      "source": [
        "for _ in env.evaluate(task, None):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt968tU5FP8g"
      },
      "source": [
        "!cat stat__v2_1010_Reacher_OWNSTUFF_6e25cc3b03badf144c111d3ae287c6cd1624905322"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tWPC9qvFP8j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}